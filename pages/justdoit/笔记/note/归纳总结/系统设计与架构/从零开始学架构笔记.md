# 架构的目的

- 架构设计的主要目的是解决**软件系统复杂度**带来的问题

# 软件系统复杂性的来源

## 高性能

## 高可用

## 可拓展性

## 低成本

## 安全

## 规模

# 架构设计原则

## 合适原则

- 合适优于业界领先

## 简单原则

- 简单优于复杂

## 演化原则

- 演化优于一步到位

# 架构设计的步骤

## 识别复杂度

## 设计备选方案

## 评估和选择备用方案

## 详细方案设计

# 高性能数据库集群

## 读写分离

- 其本质是将访问压力分散到集群中的多个节点，但是没有分散存储压力

### 主从延迟的解决方法

- 写操作后的读操作指定发给数据库主服务器
- 读从机失败后再读一次主机（二次读取）
-  关键业务读写操作全部指向主机，非关键业务采用读写分离

### 分配机制

- 程序代码封装
- 中间件封装

## 分库分表

- 既可以分散访问压力，又可以分散存储压力

### 分库的问题

- 无法join操作
- 事务问题
- 成本问题

### 分表的类型

> 类比切蛋糕

- 垂直分表（表记录数相同但包含不同的列，拆成小表）
  - 需要join
- 水平分表（表的列相同但包含不同的行数据，分水界限一般是5KW）
  - 路由（范围路由、Hash路由、配置路由表）
  - join次数增加
  - count次数增加
  - order by只能有业务代码或者中间件完成

# 高性能NoSQL

> NoSQL 方案带来的优势，本质上是牺牲 ACID 中的某个或者某几个特性，NoSQL != No SQL，而是 NoSQL = Not Only SQL

## 常见NoSQL分类

- K-V 存储：解决关系数据库无法存储数据结构的问题，以 Redis 为代表（Redis 虽然提供事务功能，但 Redis 的事务和关系数据库的事务不可同日而语，Redis 的事务只能保证隔离性和一致性（I 和 C））
- 文档数据库：解决关系数据库强 schema 约束的问题，以 MongoDB 为代表（不支持事务；无法实现关系数据库的 join 操作）
- 列式数据库：解决关系数据库大数据场景下的 I/O 问题，以 HBase 为代表（列式存储将不同列存储在磁盘上不连续的空间，导致更新多个列时磁盘是随机写操作）
- 全文搜索引擎：解决关系数据库的全文搜索性能问题，以 Elasticsearch 为代表

# 高性能缓存架构

## 缓存穿透

> **缓存穿透**是指缓存没有发挥作用，业务系统虽然去缓存查询数据，但缓存中没有数据，业务系统需要再次去存储系统查询数据

## 缓存雪崩

> **缓存雪崩**是指当缓存失效（过期）后引起系统性能急剧下降的情况

- 缓存雪崩的常见解决方法有两种：**更新锁机制**和**后台更新机制**

## 缓存热点

> 如果大部分甚至所有的业务请求都命中同一份缓存数据，则这份数据所在的缓存服务器的压力也很大

- 解决方法：复制多份缓存副本，将请求分散到多个缓存服务器上，减轻缓存热点导致的单台缓存服务器压力。缓存副本设计有一个细节需要注意，就是不同的缓存副本不要设置统一的过期时间，否则就会出现所有缓存副本同时生成同时失效的情况，从而引发缓存雪崩效应

# 单服务器高性能

## 连接与请求

- 海量连接海量请求：抢购
-  常量连接海量请求：中间件
- 海量连接常量请求：门户网站
- 常量连接常量请求：内部系统

## PPC

> PPC 是 Process Per Connection 的缩写，其含义是指每次有新的连接就新建一个进程去专门处理这个连接的请求，这是传统的 UNIX 网络服务器所采用的模型

- 缺点：fork 代价高，父子进程通信复杂，支持的并发连接数量有限
- PPC 模式中，当连接进来时才 fork 新进程来处理连接请求，由于 fork 进程代价高，用户访问时可能感觉比较慢，prefork 模式的出现就是为了解决这个问题：提前创建进程。prefork 的实现关键就是多个子进程都 accept 同一个 socket，当有新的连接进入时，操作系统保证只有一个进程能最后 accept 成功。但这里也存在一个小小的问题：“惊群”现象，就是指虽然只有一个子进程能 accept 成功，但所有阻塞在 accept 上的子进程都会被唤醒，这样就导致了不必要的进程调度和上下文切换了

## TPC

> TPC 是 Thread Per Connection 的缩写，其含义是指每次有新的连接就新建一个线程去专门处理这个连接的请求。TPC 实际上是解决或者弱化了 PPC fork 代价高的问题和父子进程通信复杂的问题

- 缺点：创建线程虽然比创建进程代价低，但并不是没有代价；无须进程间通信，但是线程间的互斥和共享又引入了复杂度；多线程会出现互相影响的情况，某个线程出现异常时，可能导致整个进程退出

- 和 prefork 类似，prethread 模式会预先创建线程，然后才开始接受用户的请求，当有新的连接进来的时候，就可以省去创建线程的操作

## Reactor

- PPC与TPC最大的问题是创建进程与线程的开销，可以引入池化技术缓解
- 为了解决阻塞问题引入非阻塞技术IO多路复用，关键点是：当多条连接共用一个阻塞对象后，进程只需要在一个阻塞对象上等待，而无须再轮询所有连接，常见的实现方式有 select、epoll、kqueue 等；当某条连接有新的数据可以处理时，操作系统会通知进程，进程从阻塞状态返回，开始进行业务处理
- I/O 多路复用结合线程池，完美地解决了 PPC 和 TPC 的问题，这就是Reactor模式，中文是“反应堆”，实际上是“事件反应”的意思。Reactor 模式也叫 Dispatcher 模式
- Reactor 模式的核心组成部分包括 Reactor 和处理资源池（进程池或线程池），其中 Reactor 负责监听和分配事件，处理资源池负责处理事件。

### 实现方案

- 单 Reactor 单进程 / 线程：单 Reactor 单进程的模式优点就是很简单，没有进程间通信，没有进程竞争，全部都在同一个进程内完成。但其缺点也是非常明显，具体表现有：只有一个进程，无法发挥多核 CPU 的性能；Handler 在处理某个连接上的业务时，整个进程无法处理其他连接的事件（Redis）
- 单 Reactor 多线程：单 Reator 多线程方案能够充分利用多核多 CPU 的处理能力，但同时也存在下面的问题：多线程数据共享和访问比较复杂；Reactor 承担所有事件的监听和响应，只在主线程中运行，瞬间高并发时会成为性能瓶颈
- 多 Reactor 多进程 / 线程（目前著名的开源系统 Nginx 采用的是多 Reactor 多进程，采用多 Reactor 多线程的实现有 Memcache 和 Netty）

## Proactor

> Reactor 是非阻塞同步网络模型，因为真正的 read 和 send 操作都需要用户进程同步操作。这里的“同步”指用户进程在执行 read 和 send 这类 I/O 操作的时候是同步的，如果把 I/O 操作改为异步就能够进一步提升性能，这就是异步网络模型 Proactor。Reactor 可以理解为“来了事件我通知你，你来处理”，而 Proactor 可以理解为“**来了事件我来处理，处理完了我通知你**”。这里的“我”就是操作系统内核，“事件”就是有新连接、有数据可读、有数据可写的这些 I/O 事件，“你”就是我们的程序代码

- 目前 Windows 下通过 IOCP 实现了真正的异步 I/O，而在 Linux 系统下的 AIO 并不完善，因此在 Linux 下实现高并发网络编程时都是以 Reactor 模式为主

# 高性能负载均衡

> **高性能集群的复杂性主要体现在需要增加一个任务分配器，以及为任务选择一个合适的任务分配算法**。对于任务分配器，现在更流行的通用叫法是“负载均衡器”

## 分类

### DNS 负载均衡

> DNS 是最简单也是最常见的负载均衡方式，一般用来实现地理级别的均衡。DNS 负载均衡的本质是 DNS 解析同一个域名可以返回不同的 IP 地址

### 硬件负载均衡

> 硬件负载均衡是通过单独的硬件设备来实现负载均衡功能，这类设备和路由器、交换机类似，可以理解为一个用于负载均衡的基础网络设备。目前业界典型的硬件负载均衡设备有两款：F5 和 A10

### 软件负载均衡

> 软件负载均衡通过负载均衡软件来实现负载均衡功能，常见的有 Nginx 和 LVS，其中 Nginx 是软件的 7 层负载均衡，LVS 是 Linux 内核的 4 层负载均衡。4 层和 7 层的区别就在于**协议**和**灵活性**，Nginx 支持 HTTP、E-mail 协议；而 LVS 是 4 层负载均衡，和协议无关，几乎所有应用都可以做，例如，聊天、数据库等

## 经典架构

> DNS 负载均衡用于实现地理级别的负载均衡；硬件负载均衡用于实现集群级别的负载均衡；软件负载均衡用于实现机器级别的负载均衡

## 负载均衡算法

### 基本分类

- 任务平分类：负载均衡系统将收到的任务平均分配给服务器进行处理，这里的“平均”可以是绝对数量的平均，也可以是比例或者权重上的平均。
- 负载均衡类：负载均衡系统根据服务器的负载来进行分配，这里的负载并不一定是通常意义上我们说的“CPU 负载”，而是系统当前的压力，可以用 CPU 负载来衡量，也可以用连接数、I/O 使用率、网卡吞吐量等来衡量系统的压力。
- 性能最优类：负载均衡系统根据服务器的响应时间来进行任务分配，优先将新任务分配给响应最快的服务器。
- Hash 类：负载均衡系统根据任务中的某些关键信息进行 Hash 运算，将相同 Hash 值的请求分配到同一台服务器上。常见的有源地址 Hash、目标地址 Hash、session id hash、用户 ID Hash 等。

### 轮询

> 负载均衡系统收到请求后，按照顺序轮流分配到服务器上。轮询是最简单的一个策略，无须关注服务器本身的状态。

### 加权轮询

> 负载均衡系统根据服务器权重进行任务分配，这里的权重一般是根据硬件配置进行静态配置

### 负载最低优先

> 负载均衡系统将任务分配给当前负载最低的服务器，这里的负载根据不同的任务类型和业务场景

### 性能最优类

> 和负载最低优先类算法类似，性能最优优先类算法本质上也是感知了服务器的状态，只是通过响应时间这个外部标准来衡量服务器状态而已

### Hash类

> 负载均衡系统根据任务中的某些关键信息进行 Hash 运算，将相同 Hash 值的请求分配到同一台服务器上，这样做的目的主要是为了满足特定的业务需求

# CAP定理

> CAP 定理（CAP theorem）又被称作布鲁尔定理（Brewer's theorem）。在一个分布式系统（指**互相连接**并**共享数据**的节点的集合）中，当涉及**读写操作**时，只能保证一致性（Consistence）、可用性（Availability）、分区容错性（Partition Tolerance）三者中的两个，另外一个必须被牺牲

## CAP讨论的对象

- 互联的，共享数据的集群：如Mysql集群是CAP探讨的对象，Memcache集群则不是CAP探讨的对象
- 涉及到读写操作的，CAP关注的是对数据的读写操作，而不是全部功能

## CAP的解析

### 一致性

- 对某个指定的客户端来说，读操作保证能够返回最新的写操作结果

### 可用性

- 非故障的节点在合理的时间内返回合理的响应（不是错误和超时的响应）

### 分区容忍性

- 当出现网络分区后，系统能够继续“履行职责”

## CAP的应用

- 虽然 CAP 理论定义是三个要素中只能取两个，但放到分布式环境下来思考，我们会发现必须选择 P（分区容忍）要素，因为网络本身无法做到 100% 可靠，有可能出故障，所以分区是一个必然的现象
- 分布式系统理论上不可能选择 CA 架构，只能选择 CP （强一致性系统）或者 AP 架构（允许返回旧的数据）

## CAP关键细节点

- CAP 关注的粒度是**数据**，而不是整个系统。在实际设计过程中，每个系统不可能只处理一种数据，而是包含多种类型的数据，有的数据必须选择 CP，有的数据必须选择 AP。

- CAP 是忽略网络延迟的。
- 正常运行情况下，不存在 CP 和 AP 的选择，可以同时满足 CA。
- 放弃并不等于什么都不做，需要为分区恢复后做准备

## ACID

- Atomicity（原子性）：一个事务中的所有操作，要么全部完成，要么全部不完成，不会在中间某个环节结束。事务在执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样。
- Consistency（一致性）：在事务开始之前和事务结束以后，数据库的完整性没有被破坏。
- Isolation（隔离性）：数据库允许多个并发事务同时对数据进行读写和修改的能力。隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。
- Durability（持久性）：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。

## BASE

> BASE 是指基本可用（Basically Available）、软状态（ Soft State）、最终一致性（ Eventual Consistency），核心思想是即使无法做到强一致性（CAP 的一致性就是强一致性），但应用可以采用适合的方式达到最终一致性

- 基本可用（Basically Available）：分布式系统在出现故障时，允许损失部分可用性，即保证核心可用
- 软状态（Soft State）：允许系统存在中间状态，而该中间状态不会影响系统整体可用性。这里的中间状态就是 CAP 理论中的数据不一致。
- 最终一致性（Eventual Consistency）：系统中的所有数据副本经过一定时间后，最终能够达到一致的状态。

# FMFA方法

- FMEA（Failure mode and effects analysis，故障模式与影响分析）又称为失效模式与后果分析、失效模式与效应分析、故障模式与后果分析等
- 在架构设计领域，FMEA 的具体分析方法是：
  - 给出初始的架构设计图。
  - 假设架构中某个部件发生故障。
  - 分析此故障对系统功能造成的影响。
  - 根据分析结果，判断架构是否需要进行优化。
- 常见的 FMEA 分析表格包含下面部分：
  - 功能点：从用户角度来看的，而不是从系统各个模块功能点划分来看的
  - 故障模式：故障模式指的是系统会出现什么样的故障，包括故障点和故障形式（故障的现象）
  - 故障影响：当发生故障模式中描述的故障时，功能点具体会受到什么影响
  - 严重程度指站在业务的角度故障的影响程度，一般分为“致命 / 高 / 中 / 低 / 无”五个档次
  - 故障原因：“故障模式”中只描述了故障的现象，并没有单独列出故障原因
  - 故障概率：指某个具体故障原因发生的概率
  - 风险程度：风险程度就是综合严重程度和故障概率来一起判断某个故障的最终等级，风险程度 = 严重程度 × 故障概率
  - 已有措施：针对具体的故障原因，系统现在是否提供了某些措施来应对，包括：检测告警、容错、自恢复
  - 规避措施：规避措施指为了降低故障发生概率而做的一些事情，可以是技术手段，也可以是管理手段
  - 解决措施：解决措施指为了能够解决问题而做的一些事情，一般都是技术手段
  - 后续规划：综合前面的分析，就可以看出哪些故障我们目前还缺乏对应的措施，哪些已有措施还不够，针对这些不足的地方，再结合风险程度进行排序，给出后续的改进规划。这些规划既可以是技术手段，也可以是管理手段；可以是规避措施，也可以是解决措施。同时需要考虑资源的投入情况，优先将风险程度高的系统隐患解决。

# 高可用储存架构

## 双机架构

### 主备复制

> 主备架构中的“备机”主要还是起到一个备份作用，并不承担实际的业务读写操作，如果要把备机改为主机，需要人工操作

### 主从复制

> 主机负责读写操作，从机只负责读操作，不负责写操作。写少读多的业务使用主从复制的存储架构比较多

### 双机切换

> 双机切换就是为了解决这两个问题而产生的，包括主备切换和主从切换两种方案。简单来说，这两个方案就是在原有方案的基础上增加“切换”功能，即系统自动决定主机角色，并完成角色切换

- 设计要点
  - 主备间状态判断
  - 切换决策
  - 数据冲突解决
- 常见架构
  - 互连式：在主备复制的架构基础上，主机和备机多了一个“状态传递”的通道，这个通道就是用来传递状态信息的
  - 中介式：中介式指的是在主备两者之外引入第三方中介，主备机之间不直接连接，而都去连接中介，并且通过中介来传递状态信息
  - 模拟式：模拟式指主备机之间并不传递任何状态数据，而是备机模拟成一个客户端，向主机发起模拟的读写操作，根据读写操作的响应情况来判断主机的状态

### 主主复制

- 两台都是主机，不存在切换的概念；客户端无须区分不同角色的主机，随便将读写操作发送给哪台主机都可以
- 如果采取主主复制架构，必须保证数据能够双向复制，而很多数据是不能双向复制的，如：用户自增ID，库存，余额

## 集群与分区

> 主备、主从、主主架构本质上都有一个隐含的假设：主机能够存储所有数据，但主机本身的存储和处理能力肯定是有极限的。集群就是多台机器组合在一起形成一个统一的系统，这里的“多台”，数量上至少是 3 台；相比而言，主备、主从都是 2 台机器。

---

> 根据集群中机器承担的不同角色来划分，集群可以分为两类：数据集中集群、数据分散集群

### 数据集中集群

- 数据集中集群与主备、主从这类架构相似，我们也可以称数据集中集群为 1 主多备或者 1 主多从。无论是 1 主 1 从、1 主 1 备，还是 1 主多备、1 主多从，数据都只能往主机中写，而读操作可以参考主备、主从架构进行灵活多变
- 关键点：主机如何将数据复制给备机；备机如何检测主机状态；主机故障后，如何决定新的主机
- 目前开源的数据集中集群以 ZooKeeper 为典型，ZooKeeper 通过 ZAB 算法来解决上述提到的几个问题，但 ZAB 算法的复杂度是很高的

### 数据分散集群

- 数据分散集群指多个服务器组成一个集群，每台服务器都会负责存储一部分数据；同时，为了提升硬件利用率，每台服务器又会备份一部分数据
- 关键点：均衡性，容错性，可伸缩性
- 数据分散集群和数据集中集群的不同点在于，数据分散集群中的每台服务器都可以处理读写请求，因此不存在数据集中集群中负责写的主机那样的角色。但在数据分散集群中，必须有一个角色来负责执行数据分配算法，这个角色可以是独立的一台服务器，也可以是集群自己选举出的一台服务器。如果是集群服务器选举出来一台机器承担数据分区分配的职责，则这台服务器一般也会叫作主机，但我们需要知道这里的“主机”和数据集中集群中的“主机”，其职责是有差异的

---

### 数据（地理）分区

> 极端灾害或者事故，可能会导致一个城市甚至一个地区的所有基础设施瘫痪，这种情况下基于硬件故障而设计的高可用架构不再适用，我们需要基于地理级别的故障来设计高可用架构，这就是数据分区架构产生的背景

- 数据量
- 分区规则：洲际分区、国家分区、城市分区
- 复制规则
  - 集中式：存在一个总的备份中心，所有的分区都将数据备份到备份中心
  - 互备式：每个分区备份另外一个分区的数据
  - 独立式：独立式备份指每个分区自己有独立的备份中心

# 计算高可用

> 计算高可用的主要设计目标是当出现部分硬件损坏时，计算任务能够继续正常运行。因此计算高可用的本质是通过冗余来规避部分故障的风险。

## 设计要点

- 哪些服务器可以执行任务
  - 每个服务器都可以执行任务
  - 只有特定服务器（通常叫“主机”）可以执行任务，当执行任务的服务器故障后，系统需要挑选新的服务器来执行任务
- 任务如何重新执行
  - 对于已经分配的任务即使执行失败也不做任何处理，系统只需要保证新的任务能够分配到其他非故障服务器上执行即可
  - 设计一个任务管理器来管理需要执行的计算任务，服务器执行完任务后，需要向任务管理器反馈任务执行结果，任务管理器根据任务执行结果来决定是否需要将任务重新分配到另外的服务器上执行

## 主备

- 根据备机状态的不同，主备架构又可以细分为冷备架构和温备架构

## 主从

- 主从架构的从机也执行任务，发挥了从机的硬件性能；主从架构需要将任务分类，任务分配器会复杂一些

## 集群

> 高可用计算的集群方案根据集群中服务器节点角色的不同，可以分为两类：一类是对称集群，即集群中每个服务器的角色都是一样的，都可以执行所有任务；另一类是非对称集群，集群中的服务器分为多个不同的角色，不同的角色执行不同的任务，例如最常见的 Master-Slave 角色

### 对称集群

- 对称集群更通俗的叫法是负载均衡集群
- 负载均衡集群的设计关键点在于两点：
  - 任务分配器需要选取分配策略
  - 任务分配器需要检测服务器状态

### 非对称集群

> 非对称集群中不同服务器的角色是不同的，不同角色的服务器承担不同的职责。以 Master-Slave 为例，部分任务是 Master 服务器才能执行，部分任务是 Slave 服务器才能执行

- 任务分配策略更加复杂：需要将任务划分为不同类型并分配给不同角色的集群节点。
- 角色分配策略实现比较复杂：例如，可能需要使用 ZAB、Raft 这类复杂的算法来实现 Leader 的选举。

# 异地多活架构

> 异地就是指地理位置上不同的地方，活”是活动、活跃的意思

- 判断一个系统是否符合异地多活，需要满足两个标准：
  - 正常情况下，用户无论访问哪一个地点的业务系统，都能够得到正确的业务服务。
  - 某个地方业务异常的时候，用户访问其他地方正常的业务系统，能够得到正确的业务服务。

## 架构模式

### 同城异区

- 同城异区指的是将业务部署在同一个城市不同区的多个机房

- 结合复杂度、成本、故障发生概率来综合考虑，同城异区是应对机房级别故障的最优架构
- 架构设计上可以将两个机房当作本地机房来设计，无须额外考虑

### 跨城异地

- 跨城异地指的是业务部署在不同城市的多个机房，而且距离最好要远一些

- 数据不一致业务肯定不会正常，但跨城异地肯定会导致数据不一致。如果是强一致性要求的数据，例如银行存款余额、支付宝余额等，这类数据实际上是无法做到跨城异地多活的
- 跨城异地多活是架构设计复杂度最高的一种

### 跨国异地

- 为不同地区用户提供服务
- 只读类业务做多活
- 对架构设计要求不高

### 双花攻击

- double spend attack

## 设计技巧

### 保证核心业务的异地多活

### 保证核心数据最终一致性

### 采用多种手段同步数据

- 消息队列方式
- 二次读取方式
- 存储系统同步方式
- 回源读取方式
- 重新生成数据方式

### 只保证绝大部分用户的异地多活

## 设计步骤

### 业务分级

> 按照一定的标准将业务进行分级，挑选出核心的业务，只为核心业务设计异地多活，降低方案整体复杂度和实现成本

- 访问量大的业务
- 核心业务
- 产生大量收入的业务

### 数据分类

> 对核心业务相关的数据进一步分析，目的在于识别所有的数据及数据特征，这些数据特征会影响后面的方案设计

- 数据量
- 唯一性
- 实时性
- 可丢失性
- 可恢复性

### 数据同步

- 存储系统同步
- 消息队列同步
- 重复生成

### 异常处理

- 多通道同步
- 同步和访问结合
- 日志记录
- 用户补偿

# 接口级别故障

## 导致接口级故障的原因

- 内部原因：程序 bug 导致死循环，某个接口导致数据库慢查询，程序逻辑不完善导致耗尽内存等
- 外部原因：黑客攻击、促销或者抢购引入了超出平时几倍甚至几十倍的用户，第三方系统大量请求，第三方系统响应缓慢等

## 降级

> 降级指系统将某些业务或者接口的功能降低，可以是只提供部分功能，也可以是完全停掉所有功能

- 系统后门降级
- 独立降级系统

## 熔断

> 降级的目的是应对系统自身的故障，而熔断的目的是应对依赖的外部系统故障的情况

- 熔断机制实现的关键是需要有一个统一的 API 调用层，由 API 调用层来进行采样或者统计，如果接口调用散落在代码各处就没法进行统一处理了

## 限流

> 降级是从系统功能优先级的角度考虑如何应对故障，而限流则是从用户访问压力的角度来考虑如何应对故障。限流指只允许系统能够承受的访问量进来，超出系统访问能力的请求将被丢弃。

- 基于请求限流：从系统外部考虑的
- 基于资源限流：从系统内部考虑的

## 排队

> 排队实际上是限流的一个变种，限流是直接拒绝用户，排队是让用户等待一段时间

- 排队模块
- 调度模块
- 服务模块

# 可拓展架构

## 可扩展的基本思想

> 所有的可扩展性架构设计，背后的基本思想都可以总结为一个字：**拆**

- 按照不同的思路来拆分软件系统，就会得到不同的架构。常见的拆分思路有如下三种。
  - 面向流程拆分：将整个业务流程拆分为几个阶段，每个阶段作为一部分。
  - 面向服务拆分：将系统提供的服务拆分，每个服务作为一部分。
  - 面向功能拆分：将系统提供的功能拆分，每个功能作为一部分。

## 可拓展方式

> 这几个系统架构并不是非此即彼的，而是可以在系统架构设计中进行组合使用的

- 不同拆分方式应对扩展时的优势
  - 面向流程拆分：扩展时大部分情况只需要修改某一层，少部分情况可能修改关联的两层，不会出现所有层都同时要修改。（分层架构）
  - 面向服务拆分：对某个服务扩展，或者要增加新的服务时，只需要扩展相关服务即可，无须修改所有的服务。（SOA、微服务）
  - 面向功能拆分：对某个功能扩展，或者要增加新的功能时，只需要扩展相关功能即可，无须修改所有的服务。（微内核架构）

## 分层架构

> 分层架构是很常见的架构模式，它也叫 N 层架构，通常情况下，N 至少是 2 层。例如，C/S 架构、B/S 架构。常见的是 3 层架构（例如，MVC、MVP 架构）

- 分层结构的这种约束，好处在于强制将分层依赖限定为两两依赖，降低了整体系统复杂度
- 分层结构的代价就是冗余，也就是说，不管这个业务有多么简单，每层都必须要参与处理，甚至可能每层都写了一个简单的包装函数。分层架构另外一个典型的缺点就是性能，因为每一次业务请求都需要穿越所有的架构分层，有一些事情是多余的，多少都会有一些性能的浪费。

## SOA

> SOA 的全称是 Service Oriented Architecture，中文翻译为“面向服务的架构”

- SOA 更多是在传统企业（例如，制造业、金融业等）落地和推广，在互联网行业并没有大规模地实践和推广
- SOA 出现 的背景是企业内部的 IT 系统重复建设且效率低下
- SOA 架构是比较高层级的架构设计理念，一般情况下我们可以说某个企业采用了 SOA 的架构来构建 IT 系统，但不会说某个独立的系统采用了 SOA 架构。

### 关键点

- 服务：所有业务功能都是一项服务，服务就意味着要对外提供开放的能力，当其他系统需要使用这项功能时，无须定制化开发
- ESB：ESB 的全称是 Enterprise Service Bus，中文翻译为“企业服务总线”，ESB 将企业中各个不同的服务连接在一起
- 松耦合：松耦合的目的是减少各个服务间的依赖和互相影响。因为采用 SOA 架构后，各个服务是相互独立运行的，甚至都不清楚某个服务到底有多少对其他服务的依赖。

# 微服务

## 微服务与SOA

> 微服务是一种和 SOA 相似但本质上不同的架构理念

- 微服务和 SOA 只是有点类似，但本质上是不同的架构设计理念。相似点在于交叉的地方，就是两者都关注“服务”，都是通过服务的拆分来解决可扩展性问题。本质上不同的地方在于几个核心理念的差异：是否有 ESB、服务的粒度、架构设计的目标等

### 服务粒度

- 整体上来说，SOA 的服务粒度要粗一些，而微服务的服务粒度要细一些

### 服务通信

- SOA 采用了 ESB 作为服务间通信的关键组件，负责服务定义、服务路由、消息转换、消息传递，总体上是重量级的实现。微服务推荐使用统一的协议和格式，例如，RESTful 协议、RPC 协议，无须 ESB 这样的重量级实现

### 服务交付

- SOA 对服务的交付并没有特殊要求，因为 SOA 更多考虑的是兼容已有的系统；微服务的架构理念要求“快速交付”，相应地要求采取自动化测试、持续集成、自动化部署等敏捷开发相关的最佳实践

### 应用场景

- SOA 更加适合于庞大、复杂、异构的企业级系统，这类系统的典型特征就是很多系统已经发展多年，采用不同的企业级技术，无法完全推倒重来或者进行大规模的优化和重构。因为成本和影响太大，只能采用兼容的方式进行处理，而承担兼容任务的就是 ESB。微服务更加适合于快速、轻量级、基于 Web 的互联网系统，这类系统业务变化快，需要快速尝试、快速交付

## 微服务的陷进

- 服务划分过细，服务间关系复杂：`n 个服务的复杂度是 n×(n-1)/2`
- 服务数量太多，团队效率急剧下降
- 调用链太长，性能下降：一般线上的业务接口之间的调用，平均响应时间大约为 50 毫秒，如果用户的一起请求需要经过 6 次微服务调用，则性能消耗就是 300 毫秒
- 调用链太长，问题定位困难：系统拆分为微服务后，一次用户请求需要多个微服务协同处理，任意微服务的故障都将导致整个业务失败。然而由于微服务数量较多，且故障存在扩散现象，快速定位到底是哪个微服务故障是一件复杂的事情
- 没有自动化支撑，无法快速交付
- 没有服务治理，微服务数量多了后管理混乱

## 服务粒度

- 三个火枪手”原则，即一个微服务三个人负责开发
- “三个火枪手”的原则主要应用于微服务设计和开发阶段，如果微服务经过一段时间发展后已经比较稳定，处于维护期了，无须太多的开发，那么平均 1 个人维护 1 个微服务甚至几个微服务都可以。当然考虑到人员备份问题，每个微服务最好都安排 2 个人维护，每个人都可以维护多个微服务

## 拆分方法

> 拆分方式不是多选一，而是可以根据实际情况自由排列组合

- 基于业务逻辑拆分：将系统中的业务模块按照职责范围识别出来，每个单独的业务模块拆分为一个独立的服务。从业务的角度来拆分的话，规模粗和规模细都没有问题，因为拆分基础都是业务逻辑，要判断拆分粒度，不能从业务逻辑角度，而要根据前面介绍的“三个火枪手”的原则，计算一下大概的服务数量范围，然后再确定合适的“职责范围”，否则就可能出现划分过粗或者过细的情况，而且大部分情况下会出现过细的情况
- 基于可扩展拆分：将系统中的业务模块按照稳定性排序，将已经成熟和改动不大的服务拆分为**稳定服务**，将经常变化和迭代的服务拆分为**变动服务**。稳定的服务粒度可以粗一些，即使逻辑上没有强关联的服务；不稳定的服务粒度可以细一些，但也不要太细，始终记住要控制服务的总数量
- 基于可靠性拆分：将系统中的业务模块按照优先级排序，将可靠性要求高的核心服务和可靠性要求低的非核心服务拆分开来，然后重点保证核心服务的高可用
- 基于性能拆分：基于性能拆分和基于可靠性拆分类似，将性能要求高或者性能压力大的模块拆分出来，避免性能压力大的服务影响其他服务

## 基础设施

### 优先级

> 3 和 4 两类基础设施，其重要性会随着微服务节点数量增加而越来越重要，但在微服务节点数量较少的时候，可以通过人工的方式支撑，虽然效率不高，但也基本能够顶住

1. 服务发现、服务路由、服务容错：这是最基本的微服务基础设施。

2. 接口框架、API 网关：主要是为了提升开发效率，接口框架是提升内部服务的开发效率，API 网关是为了提升与外部服务对接的效率。

3. 自动化部署、自动化测试、配置中心：主要是为了提升测试和运维效率。

4. 服务监控、服务跟踪、服务安全：主要是为了进一步提升运维效率。

### 服务发现

- 自理式：指每个微服务自己完成服务发现
- 代理式：指微服务之间有一个负载均衡系统（图中的 LOAD BALANCER 节点），由负载均衡系统来完成微服务之间的服务发现
- 不管是自理式还是代理式，服务发现的核心功能就是服务注册表，注册表记录了所有的服务节点的配置和状态，每个微服务启动后都需要将自己的信息注册到服务注册表，然后由微服务或者 LOAD BALANCER 系统到服务注册表查询可用服务

### 服务路由

- 有了服务发现后，微服务之间能够方便地获取相关配置信息，但具体进行某次调用请求时，我们还需要从所有符合条件的可用微服务节点中挑选出一个具体的节点发起请求
- 服务路由和服务发现紧密相关，服务路由一般不会设计成一个独立运行的系统，通常情况下是和服务发现放在一起实现的。对于自理式服务发现，服务路由是微服务内部实现的；对于代理式服务发现，服务路由是由 LOAD BALANCER 系统实现的。

### 服务容错

- 常见的服务容错包括请求重试、流控和服务隔离。
- 通常情况下，服务容错会集成在服务发现和服务路由系统中

### 接口框架

- 微服务提倡轻量级的通信方式，一般采用 HTTP/REST 或者 RPC 方式统一接口协议。但在实践过程中，光统一接口协议还不够，还需要统一接口传递的数据格式
- 接口框架不是一个可运行的系统，一般以库或者包的形式提供给所有微服务调用。例如，针对上面的 JSON 样例，可以由某个基础技术团队提供多种不同语言的解析包

### API 网关

- 系统拆分为微服务后，内部的微服务之间是互联互通的，相互之间的访问都是点对点的
- API 网关是外部系统访问的接口，所有的外部系统接⼊系统都需要通过 API 网关，主要包括接入鉴权（是否允许接入）、权限控制（可以访问哪些功能）、传输加密、请求路由、流量控制等功能

### 自动化测试

- 通过自动化测试系统来完成绝大部分测试回归的工作
- 理论上自动化测试涵盖的范围包括代码级的单元测试、单个系统级的集成测试、系统间的接口测试（接口测试是必须的）

### 自动化部署

- 自动化部署系统包括版本管理、资源管理（例如，机器管理、虚拟机管理）、部署操作、回退操作

### 配置中心

- 配置中心包括配置版本管理，增删改查配置、节点管理、配置同步、配置推送等功能

### 服务监控

- 实时搜集信息并进行分析，避免故障后再来分析，减少了处理时间
- 服务监控可以在实时分析的基础上进行预警，在问题萌芽的阶段发觉并预警，降低了问题影响的范围和时间
- 通常情况下，服务监控需要搜集并分析大量的数据，因此建议做成独立的系统，而不要集成到服务发现、API 网关等系统中

### 服务跟踪

- 服务监控可以做到微服务节点级的监控和信息收集，但如果我们需要跟踪某一个请求在微服务中的完整路径，服务监控是难以实现的。

- 目前无论是分布式跟踪还是微服务的服务跟踪，绝大部分请求跟踪的实现技术都基于 Google 的 Dapper 论文

### 服务安全

> 服务安全主要分为三部分：接入安全、数据安全、传输安全

- 通常情况下，服务安全可以集成到配置中心系统中进行实现，即配置中心配置微服务的接入安全策略和数据安全策略，微服务节点从配置中心获取这些配置信息，然后在处理具体的微服务调用请求时根据安全策略进行处理。
- 由于这些策略是通用的，一般会把策略封装成通用的库提供给各个微服务调用

# 微内核架构

> 微内核架构（Microkernel Architecture），也被称为插件化架构（Plug-in Architecture），是一种面向功能进行拆分的可扩展性架构，通常用于实现基于产品的应用（如 Eclipse 这类 IDE 软件、UNIX 这类操作系统、淘宝 App 这类客户端软件）

## 基本架构

> 微内核架构包含两类组件：核心系统（core system）和插件模块（plug-in modules）

- 核心系统：负责和具体业务功能无关的通用功能，例如模块加载、模块间通信等
- 插件模块：负责实现具体的业务逻辑
- 上面这张图中核心系统 Core System 功能比较稳定，不会因为业务功能扩展而不断修改，插件模块可以根据业务功能的需要不断地扩展。微内核的架构本质就是将变化部分封装在插件里面，从而达到快速灵活扩展的目的，而又不影响整体系统的稳定

## 设计要点

- 插件管理：核心系统需要知道当前有哪些插件可用，如何加载这些插件，什么时候加载插件。常见的实现方法是插件注册表机制
- 插件连接：插件连接指插件如何连接到核心系统。常见的连接机制有 OSGi（Eclipse 使用）、消息模式、依赖注入（Spring 使用），甚至使用分布式的协议都是可以的，比如 RPC 或者 HTTP Web 的方式
- 插件通信：插件通信指插件间的通信。虽然设计的时候插件间是完全解耦的，但实际业务运行过程中，必然会出现某个业务流程需要多个插件协作，这就要求两个插件间进行通信
- 由于插件之间没有直接联系，通信必须通过核心系统，因此核心系统需要提供插件通信机制（这种情况和计算机类似）

## OSGi 架构简析

> OSGi 的全称是 Open Services Gateway initiative

### 模块层（Module 层）

- 模块层实现插件管理功能。OSGi 中，插件被称为 Bundle，每个 Bundle 是一个 Java 的 JAR 文件，每个 Bundle 里面都包含一个元数据文件 MANIFEST.MF，这个文件包含了 Bundle 的基本信息

### 生命周期层（Lifecycle 层）

- 生命周期层实现插件连接功能，提供了执行时模块管理、模块对底层 OSGi 框架的访问。生命周期层精确地定义了 Bundle 生命周期的操作（安装、更新、启动、停止、卸载），Bundle 必须按照规范实现各个操作

### 服务层（Service 层）

- 服务层实现插件通信的功能。OSGi 提供了一个服务注册的功能，用于各个插件将自己能提供的服务注册到 OSGi 核心的服务注册中心，如果某个服务想用其他服务，则直接在服务注册中心搜索可用服务中心就可以了

## 规则引擎架构简析

> 规则引擎从结构上来看也属于微内核架构的一种具体实现，其中执行引擎可以看作是微内核，执行引擎解析配置好的业务流，执行其中的条件和规则，通过这种方式来支持业务的灵活多变（规则引擎在计费、保险、促销等业务领域应用较多）

# 技术演进

- 潮流派：对于新技术特别热衷，紧跟技术潮流，当有新的技术出现时，迫切想将新的技术应用到自己的产品中
- 保守派：对于新技术抱有很强的戒备心，稳定压倒一切，已经掌握了某种技术，就一直用这种技术打天下
- 跟风派：跟风派与潮流派不同，这里的跟风派不是指跟着技术潮流，而是指跟着竞争对手的步子走

## 技术演进的动力

- 影响一个企业业务的发展主要有 3 个因素：市场、技术、管理，这三者构成支撑业务发展的铁三角，任何一个因素的不足，都可能导致企业的业务停滞不前
- 我们可以简单地将企业的业务分为两类：一类是产品类，一类是服务类
  - 产品类：杀毒软件、浏览器
  - 服务类：淘宝

- 除非是开创新的技术能够推动或者创造一种新的业务，其他情况下，都是业务的发展推动了技术的发展

## 技术演进的模式

- **基于业务发展阶段进行判断**，这也是为什么架构师必须具备业务理解能力的原因。不同的行业业务发展路径、轨迹、模式不一样，架构师必须能够基于行业发展和企业自身情况做出准确判断

## 业务复杂性

- 初创期
- 发展期
- 竞争期
- 成熟期

## 用户规模

- 性能
- 可用性

# 互联网架构模板

- ![img](https://static001.geekbang.org/resource/image/40/36/4032179c7ca698986cb57135d9c69b36.png)

## 储存层

### SQL

- SQL 即我们通常所说的关系数据

## NoSQL

- NoSQL 不是 No SQL，而是 Not Only SQL，即 NoSQL 是 SQL 的补充
- NoSQL 发展到一定规模后，通常都会在 NoSQL 集群的基础之上再实现统一**存储平台**，统一存储平台主要实现这几个功能
  - 资源动态按需动态分配
  - 资源自动化管理
  - 故障自动化处理

### 小文件存储

- HBase、Hadoop、Hypertable、FastDFS 等都可以作为小文件存储的底层平台，只需要将这些开源方案再包装一下基本上就可以用了

### 大文件储存

- 互联网行业的大文件主要分为两类：一类是业务上的大数据，如电影；另一类是海量的日志数据。
- Google 的 3 篇大数据论文（Bigtable/Map- Reduce/GFS）开启了一个大数据的时代，而 Yahoo 开源的 Hadoop 系列（HDFS、HBase 等），基本上垄断了开源界的大数据处理

## 开发层

### 开发框架

- Java 相关的开发框架 SSH、SpringMVC、Play，Ruby 的 Ruby on Rails，PHP 的 ThinkPHP，Python 的 Django 等

### Web服务器

- Java 的有 Tomcat、JBoss、Resin 等，PHP/Python 的用 Nginx，当然最保险的就是用 Apache 了，什么语言都支持

### 容器

- 传统的虚拟化技术是虚拟机，解决了跨平台的问题，但由于虚拟机太庞大，启动又慢，运行时太占资源，在互联网行业并没有大规模应用；而 Docker 的容器技术，虽然没有跨平台，但启动快，几乎不占资源

## 服务层

### 配置中心

### 服务中心

- 当系统数量不多的时候，系统间的调用一般都是直接通过配置文件记录在各系统内部的，但当系统数量多了以后，这种方式就存在问题了。服务中心就是为了解决上面提到的跨系统依赖的“配置”和“调度”问题
- 服务名字系统（Service Name System）：服务名字系统是为了将 Service 名称解析为“host + port + 接口名称”，但是和 DNS 一样，真正发起请求的还是请求方
- 服务总线系统（Service Bus System）：由总线系统完成调用，服务请求方都不需要直接和服务提供方交互了

### 消息队列

- 引入消息队列系统后的效果：
  - 整体结构从网状结构变为线性结构，结构清晰。
  - 消息生产和消息消费解耦，实现简单。
  - 增加新的消息消费者，消息生产者完全不需要任何改动，扩展方便。
  - 消息队列系统可以做高可用、高性能，避免各业务子系统各自独立做一套，减轻工作量。
  - 业务子系统只需要聚焦业务即可，实现简单

## 网络层

### 负载均衡

- DNS：地理级别的负载均衡
- Nginx 、LVS 、F5：同一地点内机器级别的负载均衡

- CDN：解决用户网络访问时的“最后一公里”效应，本质上是一种“以空间换时间”的加速策略，即将内容缓存在离用户最近的地方，用户访问的是缓存的内容，而不是站点实时的内容

### 多机房

-  同城多机房
- 跨城多机房
- 跨国多机房

### 多中心

> 多中心必须以多机房为前提，但从设计的角度来看，多中心相比多机房是本质上的飞越，难度也高出一个等级

- 多机房的主要目标是灾备，当机房故障时，可以比较快速地将业务切换到另外一个机房，这种切换操作允许一定时间的中断
- 多中心要求每个中心都同时对外提供服务，且业务能够自动在多中心之间切换，故障后不需人工干预或者很少的人工干预就能自动恢复

## 用户层

### 用户管理

- 用户管理的第一个目标：**单点登录（SSO）**，又叫统一登录。单点登录的技术实现手段较多，例如 cookie、JSONP、token 等，目前最成熟的开源单点登录方案当属 CAS
- 用户管理的第二个目标：**授权登录**。现在最流行的授权登录就是 OAuth 2.0 协议，基本上已经成为了事实上的标准

### 消息推送

> 消息推送主要包含 3 个功能：设备管理（唯一标识、注册、注销）、连接管理和消息管理

- 海量设备和用户管理﻿
- 连接保活﻿
- 消息管理﻿

### 存储云、图片云

- 普通的文件基本上提供存储和访问就够了，而图片涉及的业务会更多，包括裁剪、压缩、美化、审核、水印等处理，因此通常情况下图片云会拆分为独立的系统对用户提供服务

## 业务层

- 复杂度越来越高的一个主要原因就是系统越来越庞大，业务越来越多。解决方法是化整为零、分而治之，将整体复杂性分散到多个子业务或者子系统里面去
- 随着子系统数量越来越多，如果达到几百上千，另外一个复杂度问题又会凸显出来：子系统数量太多，已经没有人能够说清楚业务的调用流程了，出了问题排查也会特别复杂。
- “合”的方式是按照“高内聚、低耦合”的原则，将职责关联比较强的子系统合成一个**虚拟业务域**，然后通过网关对外统一呈现，类似于设计模式中的 Facade 模式

## 平台技术

### 运维平台

- 标准化
- 平台化
- 自动化
- 可视化

### 测试平台

- 用例管理
- 资源管理
- 任务管理
- 数据管理

### 数据平台

- 数据管理：数据采集、数据存储、数据访问、数据安全
- 数据分析：数据统计、数据挖掘、机器学习、深度学习（机器学习和深度学习属于数据挖掘的一种具体实现方式，由于其实现方式与传统的数据挖掘方式差异较大，因此数据平台在实现机器学习和深度学习时，需要针对机器学习和深度学习独立进行设计）
- 数据应用

## 管理平台

- 身份认证
- 权限控制

# 架构重构

## 有的放矢

> 相比全新的架构设计来说，架构重构对架构师的要求更高。主要因为：业务已经上线，不能停下来；关联方众多，牵一发动全身；旧架构的约束

- 判断到底是采取架构重构还是采取系统优化：假设我们现在需要从 0 开始设计当前系统，新架构和老架构是否类似？如果差异不大，说明采取系统优化即可；如果差异很大，那可能就要进行系统重构了

## 合纵连横

- 合纵：将技术语言转换为通俗语言，以事实说话，以数据说话，是沟通的关键。（数据可视化提高与非技术人员的沟通效率）
- 连横：对技术人员换位思考

## 纵横帷幄

- 一般步骤
  - 救火（优化）
  - 组件化（再次优化，做铺垫）
  - 解耦（真正的重构）
- 分段实施
  - 优先级排序
  - 问题分类
  - 先易后难
  - 循序渐进

# 开源项目选择使用及二次开发

> 软件开发领域有一个流行的原则：DRY，Don’t repeat yourself。翻译过来更通俗易懂：**不要重复造轮子**

## 如何选择一个开源项目

- 聚焦是否满足业务
- 聚焦是否成熟
  - 版本号：除非特殊情况，否则不要选 0.X 版本的，至少选 1.X 版本的，版本号越高越好。
  - 使用的公司数量：一般开源项目都会把采用了自己项目的公司列在主页上，公司越大越好，数量越多越好。
  - 社区活跃度：看看社区是否活跃，发帖数、回复数、问题处理速度等。
- 聚焦运维能力
  - 开源项目日志是否齐全
  - 开源项目是否有命令行、管理控制台等维护工具
  - 开源项目是否有故障检测和恢复的能力

## 如何使用开源项目

- 深入研究，仔细测试
- 小心应用，灰度发布
- 做好应急，以防万一

## 如何基于开源项目做二次开发

- 保持纯洁，加以包装：建议是不要改动原系统，而是要开发辅助系统：监控、报警、负载均衡、管理等
- 发明你要的轮子

# App架构演进

## Web App

> Web App 架构又叫包壳架构，简单来说就是在 Web 的业务上包装一个 App 的壳，业务逻辑完全还是 Web 实现，App 壳完成安装的功能，让用户看起来像是在使用 App，实际上和用浏览器访问 PC 网站没有太大差别

- 主要解决“快速开发”和“低成本”两个复杂度问题，架构设计遵循“合适原则”和“简单原则” 

## 原生 App

- 移动开发的复杂度从“快速开发”和“低成本”转向了“用户体验”，而要保证用户体验，采用原生 App 的架构是最合适的，这里的架构设计遵循“演化原则”

## Hybrid App

- 由于 Android、iOS、Windows Phone，的原生开发完全不能兼容，同样的功能需要三个平台重复开发
- 对体验要求高的业务采用原生 App 实现，对体验要求不高的可以采用 Web 的方式实现，这就是 Hybrid App 架构的核心设计思想，主要遵循架构设计的“合适原则”

## 组件化&容器化

- 将超级 App 拆分为众多组件，这些组件遵循预先制定好的规范，独立开发、独立测试、独立上线。
- 如果某个组件依赖其他组件，组件之间通过消息系统进行通信，通过这种方式来实现组件隔离，从而避免各个团队之间的互相依赖和影响，以提升团队开发效率和整个系统的可扩展性

## 跨平台 App

- 比较知名的有 Facebook 的 React Native、阿里的 Weex、Google 的 Flutter。虽然也有很多公司在尝试使用，但目前这几个方案都不算很成熟，且在用户体验方面与原生 App 还是有一定差距

# 架构设计文档

