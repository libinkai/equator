# 概述

- 并发编程可以总结为三个核心问题：分工、同步、互斥。所谓**分工**指的是如何高效地拆解任务并分配给线程，而**同步**指的是线程之间如何协作，**互斥**则是保证同一时刻只允许一个线程访问共享资源。JDK 并发包很大部分内容都是按照这三个维度组织的，例如 Fork/Join 框架就是一种分工模式，`CountDownLatch `就是一种典型的同步方式，而可重入锁则是一种互斥手段

# 可见性、原子性、有序性

> 为了合理利用 CPU 的高性能，平衡CPU、内存、I/O 设备三者的速度差异，计算机体系机构、操作系统、编译程序都做出了贡献，主要体现为：
>
> 1. CPU 增加了缓存，以均衡与内存的速度差异；
> 2. 操作系统增加了进程、线程，以分时复用 CPU，进而均衡 CPU 与 I/O 设备的速度差异；
> 3. 编译程序优化指令执行次序，使得缓存能够得到更加合理地利用。

## 可见性

> 一个线程对共享变量的修改，另外一个线程能够立刻看到，我们称为**可见性**

- 在单核时代，所有的线程都是在一颗 CPU 上执行，CPU 缓存与内存的数据可见性容易解决。因为所有线程都是操作同一个 CPU 的缓存，一个线程对缓存的写，对另外一个线程来说一定是可见的
- 多核时代，每颗 CPU 都有自己的缓存，这时 CPU 缓存与内存的数据一致性就没那么容易解决了，当多个线程在不同的 CPU 上执行时，这些线程操作的是不同的 CPU 缓存。

## 原子性

> 我们把一个或者多个操作在 CPU 执行的过程中不被中断的特性称为**原子性**。CPU 能保证的原子操作是 CPU 指令级别的，而不是高级语言的操作符，这是违背我们直觉的地方

- 早期的操作系统基于进程来调度 CPU，不同进程间是不共享内存空间的，所以进程要做任务切换就要切换内存映射地址，而一个进程创建的所有线程，都是共享一个内存空间的，所以线程做任务切换成本就很低了。现代的操作系统都基于更轻量的线程来调度，现在我们提到的“任务切换”都是指“线程切换”

## 有序性

> **有序性**指的是程序按照代码的先后顺序执行。编译器为了优化性能，有时候会改变程序中语句的先后顺序

- 顺序性导致的问题是：在 Java 领域一个经典的案例就是利用双重检查创建单例对象

# Java内存模型

> Java 内存模型规范了 JVM 如何提供按需禁用缓存和编译优化的方法。具体来说，这些方法包括 **volatile**、**synchronized** 和 **final** 三个关键字，以及六项 **Happens-Before 规则**，主要解决了可见性与有序性得问题

## volatile

> volatile 关键字并不是 Java 语言的特产，古老的 C 语言里也有，它最原始的意义就是禁用 CPU 缓存

## synchronized

## final

- 构造函数的错误重排导致线程可能看到 final 变量的值会变化，在 1.5 以后 Java 内存模型对 final 类型变量的重排进行了约束。现在只要我们提供正确构造函数没有“逸出”，就不会出问题了

## Happens-Before 规则

> Happens-Before 并不是说前面一个操作发生在后续操作的前面，它真正要表达的是：**前面一个操作的结果对后续操作是可见的**。Happens-Before 约束了编译器的优化行为，虽允许编译器优化，但是要求编译器优化后一定遵守 Happens-Before 规则

### 程序顺序性规则

> 按照程序顺序，前面的操作 Happens-Before 于后续的任意操作

### volatile 变量规则

> 对一个 volatile 变量的写操作， Happens-Before 于后续对这个 volatile 变量的读操作

### 传递性

> 如果 A Happens-Before B，且 B Happens-Before C，那么 A Happens-Before C

### 管程中锁的规则

> 对一个锁的解锁 Happens-Before 于后续对这个锁的加锁（**管程**是一种通用的同步原语，在 Java 中指的就是 synchronized，synchronized 是 Java 里对管程的实现，管程中的锁在 Java 里是隐式实现的）

### 线程start规则

> 线程 A 启动子线程 B 后，子线程 B 能够看到主线程在启动子线程 B 前的操作

### 线程join规则

> 主线程 A 等待子线程 B 完成（主线程 A 通过调用子线程 B 的 join() 方法实现），当子线程 B 完成后（主线程 A 中 join() 方法返回），主线程能够看到子线程的操作（如果在线程 A 中，调用线程 B 的 join() 并成功返回，那么线程 B 中的任意操作 Happens-Before 于该 join() 操作的返回）

# 互斥锁

## 锁的对象

- 当修饰静态方法的时候，锁定的是当前类的 Class对象
- 当修饰非静态方法的时候，锁定的是当前实例对象this

## 锁与资源的关系

- 受保护资源和锁之间的关联关系是 N:1 的关系，多把锁关联一个受保护资源，可能导致并发问题

### 保护多个没有关联的资源

- 可以使用一把锁，也可以使用与资源数对于的锁。**用不同的锁对受保护资源进行精细化管理，能够提升性能**。这种锁还有个名字，叫**细粒度锁**

### 保护多个关联的资源

- 相当于事务

- 使用类锁（串行化）

  ```
  class Account {
    private int balance;
    // 转账
    void transfer(Account target, int amt){
      synchronized(Account.class) {
        if (this.balance > amt) {
          this.balance -= amt;
          target.balance += amt;
        }
      }
    } 
  }
  ```

- 双锁（细粒度，但是有可能导致死锁）

  ```
  class Account {
    private int balance;
    // 转账
    void transfer(Account target, int amt){
      // 锁定转出账户
      synchronized(this) {              
        // 锁定转入账户
        synchronized(target) {           
          if (this.balance > amt) {
            this.balance -= amt;
            target.balance += amt;
          }
        }
      }
    } 
  }
  ```

## 死锁

> 一组互相竞争资源的线程因互相等待，导致“永久”阻塞的现象

### 出现死锁的条件以及解决

> 只要我们破坏其中一个，就可以成功避免死锁的发生

1. **互斥**，共享资源 X 和 Y 只能被一个线程占用；
2. **占有且等待**，线程 T1 已经取得共享资源 X，在等待共享资源 Y 的时候，不释放共享资源 X（我们可以一次性申请所有的资源，这样就不存在等待了，自由）
3. **不可抢占**，其他线程不能强行抢占线程 T1 占有的资源（占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源，这样不可抢占这个条件就破坏掉了。Java 在语言层次没有解决这个问题，不过在 SDK 层面还是解决了的， JUC包下面提供的 Lock 是可以轻松解决这个问题的）
4. **循环等待**，线程 T1 等待线程 T2 占有的资源，线程 T2 等待线程 T1 占有的资源，就是循环等待（可以靠按序申请资源来预防。所谓按序申请，是指资源是有线性顺序的，申请的时候可以先申请资源序号小的，再申请资源序号大的，这样线性化后自然就不存在循环了）

## 利用等待通知机制优化自旋

> 如果线程要求的条件不满足，则线程阻塞自己，进入**等待**状态；当线程要求的条件满足后，**通知**等待的线程重新执行。其中使用线程阻塞的方式就能避免循环等待消耗 CPU 的问题

-  Java 语言内置的 synchronized 配合 `wait()、notify()、notifyAll() `这三个方法能实现等待通知机制
- 尽量使用`notifyAll()`方法，避免有些线程永远不会被通知到。使用notify的场景
  - 所有等待线程拥有相同的等待条件；
  - 所有等待线程被唤醒后，执行相同的操作；
  - 只需要唤醒一个线程

### wait与sleep的区别

- wait会释放所有锁而sleep不会释放锁资源
- wait只能在同步方法和同步块中使用，而sleep任何地方都可以
- wait无需捕捉异常，而sleep需要

## 安全性

> 线程安全本质上就是正确性，而正确性的含义就是**程序按照我们期望的执行**，不要让我们感到意外

- 数据竞争，指的是当多个线程同时访问同一数据，并且至少有一个线程会写这个数据

- 竞态条件，指的是程序的执行结果依赖线程执行的顺序（某些方法单独执行的时候没有并发正确性问题，并不代表把它的操作组合在一起问没有）
- 数据竞争和竞态条件问题这两类问题，都可以用**互斥**这个技术方案解决

## 活跃性

> 活跃性问题，指的是某个操作无法执行下去。我们常见的“死锁”就是一种典型的活跃性问题，除了死锁外，还有两种情况，分别是活锁和饥饿

- 活锁：有时线程虽然没有发生阻塞，但仍然会存在执行不下去的情况；解决方法可以等待随机的时间
- 饥饿：线程因无法访问所需资源而无法执行下去的情况（解决方法一是保证资源充足，二是公平地分配资源，三就是避免持有锁的线程长时间执行）
- 在并发编程里，主要是使用公平锁实现公平地分配资源。所谓公平锁，是一种先来后到的方案，线程的等待是有顺序的，排在等待队列前面的线程会优先获得资源

## 性能问题

> 使用“锁”要非常小心，但是如果小心过度，也可能出性能问题。锁的过度使用可能导致串行化的范围过大，这样就不能够发挥多线程的优势了

- 阿姆达尔（Amdahl）定律：处理器并行运算之后效率提升的能力，即`S=1/((1-p)+p/n)`。解读：n 可以理解为 CPU 的核数，p 可以理解为并行百分比，那（1-p）就是串行百分比了，也就是我们假设的 5%。我们再假设 CPU 的核数（也就是 n）无穷大，那加速比 S 的极限就是 20。也就是说，如果我们的串行率是 5%，那么我们无论采用什么技术，最高也就只能提高 20 倍的性能

### 解决方法

- 既然使用锁会带来性能问题，那最好的方案自然就是使用无锁的算法和数据结构了。在这方面有很多相关的技术，例如线程本地存储 (Thread Local Storage, TLS)、写入时复制 (Copy-on-write)、乐观锁等；Java 并发包里面的原子类也是一种无锁的数据结构；Disruptor 则是一个无锁的内存队列，性能都非常好……

- 减少锁持有的时间。互斥锁本质上是将并行的程序串行化，所以要增加并行度，一定要减少持有锁的时间。这个方案具体的实现技术也有很多，例如使用细粒度的锁，一个典型的例子就是 Java 并发包里的 ConcurrentHashMap，它使用了所谓分段锁的技术；还可以使用读写锁，也就是读是无锁的，只有写的时候才会互斥。

### 性能指标

1. 吞吐量：指的是单位时间内能处理的请求数量。吞吐量越高，说明性能越好
2. 延迟：指的是从发出请求到收到响应的时间。延迟越小，说明性能越好
3. 并发量：指的是能同时处理的请求数量，一般来说随着并发量的增加、延迟也会增加。所以延迟这个指标，一般都会是基于并发量来说的。例如并发量是 1000 的时候，延迟是 50 毫秒

# 管程

> **管程和信号量是等价的，所谓等价指的是用管程能够实现信号量，也能用信号量实现管程**。但是管程更容易使用，所以 Java 选择了管程。
>
> **管程，指的是管理共享变量以及对共享变量的操作过程，让他们支持并发**

- 先后出现过三种不同的管程模型，分别是：Hasen 模型、Hoare 模型和 MESA 模型。其中，现在广泛应用的是 MESA 模型，并且 Java 管程的实现参考的也是 MESA 模型。
  - Hasen 模型、Hoare 模型和 MESA 模型的一个核心区别就是当条件满足后，如何通知相关线程。管程要求同一时刻只允许一个线程执行，那当线程 T2 的操作使线程 T1 等待的条件满足时，T1 和 T2 究竟谁可以执行呢
    1. Hasen 模型里面，**要求 notify() 放在代码的最后**，这样 T2 通知完 T1 后，T2 就结束了，然后 T1 再执行，这样就能保证同一时刻只有一个线程执行。
    2. Hoare 模型里面，T2 通知完 T1 后，T2 阻塞，T1 马上执行；等 T1 执行完，再唤醒 T2，也能保证同一时刻只有一个线程执行。但是相比 Hasen 模型，T2 多了一次阻塞唤醒操作。
    3. MESA 管程里面，T2 通知完 T1 后，T2 还是会接着执行，T1 并不立即执行，仅仅是从条件变量的等待队列进到入口等待队列里面。这样做的好处是 notify() 不用放到代码的最后，T2 也没有多余的阻塞唤醒操作。但是也有个副作用，就是当 T1 再次执行的时候，可能**曾经**满足的条件，现在已经不满足了，所以**需要以循环方式检验条件变量**。
- 管程解决互斥问题的思路很简单，就是将共享变量及其对共享变量的操作统一封装起来（管程模型和面向对象高度契合的）。可以认为管程封装了一个队列，队列的内容是线程，该队列的操作是线程安全的
- 在管程模型里，共享变量和对共享变量的操作是被封装起来的，管程只有一个入口，并且还有一个入口等待队列。当多个线程同时试图进入管程内部时，只允许一个线程进入，其他线程则在入口等待队列中等待。管程里还引入了条件变量的概念，而且**每个条件变量都对应有一个等待队列**。

- 对于 MESA 管程来说，有一个编程范式，就是需要在一个 while 循环里面调用 wait()，**这个是 MESA 管程特有的**

# 线程的生命周期

## 通用的线程生命周期

- 五态模型，这五态分别是：**初始状态、可运行状态、运行状态、休眠状态**和**终止状态**
- 这五种状态在不同编程语言里会有简化合并。例如，C 语言的 POSIX Threads 规范，就把初始状态和可运行状态合并了；Java 语言里则把可运行状态和运行状态合并了，这两个状态在操作系统调度层面有用，而 JVM 层面不关心这两个状态，因为 JVM 把线程调度交给操作系统处理了。除了简化合并，这五种状态也有可能被细化，比如，Java 语言里就细化了休眠状态

## Java线程生命周期

> 在操作系统层面，Java 线程中的 BLOCKED、WAITING、TIMED_WAITING 是一种状态，即前面我们提到的休眠状态。也就是说**只要 Java 线程处于这三种状态之一，那么这个线程就永远没有 CPU 的使用权**

- NEW（初始化状态）
- RUNNABLE（可运行 / 运行状态）
- BLOCKED（阻塞状态）
- WAITING（无时限等待）
- TIMED_WAITING（有时限等待）
- TERMINATED（终止状态）

### RUNNABLE 与 BLOCKED 的状态转换

- 只有一种场景会触发这种转换，就是线程等待 synchronized 的隐式锁。synchronized 修饰的方法、代码块同一时刻只允许一个线程执行，其他线程只能等待，这种情况下，等待的线程就会从 RUNNABLE 转换到 BLOCKED 状态。而当等待的线程获得 synchronized 隐式锁时，就又会从 BLOCKED 转换到 RUNNABLE 状态。
- 线程调用阻塞式 API 时，是否会转换到 BLOCKED 状态呢？在操作系统层面，线程是会转换到休眠状态的，但是在 JVM 层面，Java 线程的状态不会发生变化，也就是说 Java 线程的状态会依然保持 RUNNABLE 状态。**JVM 层面并不关心操作系统调度相关的状态**，因为在 JVM 看来，等待 CPU 使用权（操作系统层面此时处于可执行状态）与等待 I/O（操作系统层面此时处于休眠状态）没有区别，都是在等待某个资源，所以都归入了 RUNNABLE 状态。而我们平时所谓的 Java 在调用阻塞式 API 时，线程会阻塞，指的是操作系统线程的状态，并不是 Java 线程的状态

### RUNNABLE 与 WAITING 的状态转换

- 获得 synchronized 隐式锁的线程，调用无参数的 Object.wait() 方法
- 调用无参数的 Thread.join() 方法
- 调用 LockSupport.park() 方法

### RUNNABLE 与 TIMED_WAITING 的状态转换

- 调用**带超时参数**的 Thread.sleep(long millis) 方法；
- 获得 synchronized 隐式锁的线程，调用**带超时参数**的 Object.wait(long timeout) 方法；
- 调用**带超时参数**的 Thread.join(long millis) 方法；
- 调用**带超时参数**的 LockSupport.parkNanos(Object blocker, long deadline) 方法；
- 调用**带超时参数**的 LockSupport.parkUntil(long deadline) 方法。

### 从 NEW 到 RUNNABLE 状态

> NEW 状态的线程，不会被操作系统调度，因此不会执行。Java 线程要执行，就必须转换到 RUNNABLE 状态。从 NEW 状态转换到 RUNNABLE 状态很简单，只要调用线程对象的 start() 方法就可以了

- 创建 Thread 对象主要有两种方法
  - 一种是继承 Thread 对象，重写 run() 方法
  - 另一种是实现 Runnable 接口，重写 run() 方法，并将该实现类作为创建 Thread 对象的参数

### 从 RUNNABLE 到 TERMINATED 状态

- 线程执行完 run() 方法后，会自动转换到 TERMINATED 状态，当然如果执行 run() 方法的时候异常抛出，也会导致线程终止

- **stop() 和 interrupt() 方法的区别**

  - stop() 方法会真的杀死线程，不给线程喘息的机会，如果线程持有 ReentrantLock 锁，被 stop() 的线程并不会自动调用 ReentrantLock 的 unlock() 去释放锁，那其他线程就再也没机会获得 ReentrantLock 锁

  - interrupt() 方法仅仅是通知线程，线程有机会执行一些后续操作，同时也可以无视这个通知。被 interrupt 的线程，是怎么收到通知的呢？一种是异常，另一种是主动检测

    - 当线程 A 处于 WAITING、TIMED_WAITING 状态时，如果其他线程调用线程 A 的 interrupt() 方法，会使线程 A 返回到 RUNNABLE 状态，同时线程 A 的代码会触发 InterruptedException 异常。上面我们提到转换到 WAITING、TIMED_WAITING 状态的触发条件，都是调用了类似 wait()、join()、sleep() 这样的方法，我们看这些方法的签名，发现都会 throws InterruptedException 这个异常。这个异常的触发条件就是：其他线程调用了该线程的 interrupt() 方法。

      当线程 A 处于 RUNNABLE 状态时，并且阻塞在 java.nio.channels.InterruptibleChannel 上时，如果其他线程调用线程 A 的 interrupt() 方法，线程 A 会触发 java.nio.channels.ClosedByInterruptException 这个异常；而阻塞在 java.nio.channels.Selector 上时，如果其他线程调用线程 A 的 interrupt() 方法，线程 A 的 java.nio.channels.Selector 会立即返回。

    - 如果线程处于 RUNNABLE 状态，并且没有阻塞在某个 I/O 操作上，例如中断计算圆周率的线程 A，这时就得依赖线程 A 主动检测中断状态了。如果其他线程调用线程 A 的 interrupt() 方法，那么线程 A 可以通过 isInterrupted() 方法，检测是不是自己被中断了

# 线程数量最佳实践

> 操作系统已经解决了磁盘和网卡的利用率问题，利用中断机制还能避免 CPU 轮询 I/O 状态，也提升了 CPU 的利用率。但是操作系统解决硬件利用率问题的对象往往是单一的硬件设备，而我们的并发程序，往往需要 CPU 和 I/O 设备相互配合工作，也就是说，**我们需要解决 CPU 和 I/O 设备综合利用率的问题**

- 在单核时代，多线程主要就是用来平衡 CPU 和 I/O 设备的。如果程序只有 CPU 计算，而没有 I/O 操作的话，多线程不但不会提升性能，还会使性能变得更差，原因是增加了线程切换的成本。但是在多核时代，这种纯计算型的程序也可以利用多线程来提升性能，即并行计算

- 对于 CPU 密集型计算，多线程本质上是提升多核 CPU 的利用率，所以对于一个 4 核的 CPU，每个核一个线程，理论上创建 4 个线程就可以了，再多创建线程也只是增加线程切换的成本。所以，**对于 CPU 密集型的计算场景，理论上“线程的数量 =CPU 核数”就是最合适的**。不过在工程上，**线程的数量一般会设置为“CPU 核数 +1”**，这样的话，当线程因为偶尔的内存页失效或其他原因导致阻塞时，这个额外的线程可以顶上，从而保证 CPU 的利用率。

- 对于 I/O 密集型计算场景，最佳的线程数是与程序中 CPU 计算和 I/O 操作的耗时比相关的，我们可以总结出这样一个公式：

  > 最佳线程数 =CPU 核数 * [ 1 +（I/O 耗时 / CPU 耗时）]

- 令 R=I/O 耗时 / CPU 耗时，可以这样理解：当线程 A 执行 IO 操作时，另外 R 个线程正好执行完各自的 CPU 计算。这样 CPU 的利用率就达到了 100%

# 局部变量为什么是线程安全的

- CPU通过 CPU 的堆栈寄存器，去找到调用方法的参数和返回地址
- 每个方法在调用栈里都有自己的独立空间，称为**栈帧**，局部变量保存在线程各自的调用栈里面，不会共享，所以自然也就没有并发问题
- 方法里的局部变量，因为不会和其他线程共享，所以没有并发问题，这个思路很好，已经成为解决并发问题的一个重要技术，同时个名字叫做**线程封闭**，比较官方的解释是：**仅在单线程内访问数据**

# 面向对象与并发

## 封装共享变量

- 将共享变量作为对象属性封装在内部，对所有公共方法制定并发访问策略。对于这些不会发生变化的共享变量，建议你用 final 关键字来修饰。这样既能避免并发问题，也能很明了地表明你的设计意图

## 识别共享变量间的约束条件

- 这些约束条件，决定了并发访问策略（具体的业务需求）

- 不能简单地做条件判断，当你看到代码里出现 if 语句的时候，就应该立刻意识到可能存在竞态条件（所以有必要加锁的时候，还是得加锁）

## 制定并发访问策略

- 避免共享：避免共享的技术主要是利于线程本地存储以及为每个任务分配独立的线程
- 不变模式：这个在 Java 领域应用的很少，但在其他领域却有着广泛的应用，例如 Actor 模式、CSP 模式以及函数式编程的基础都是不变模式
- 管程及其他同步工具：Java 领域万能的解决方案是管程，但是对于很多特定场景，使用 Java 并发包提供的读写锁、并发容器等同步工具会更好

## 原则

1. 优先使用成熟的工具类：Java SDK 并发包里提供了丰富的工具类，基本上能满足你日常的需要，建议你熟悉它们，用好它们，而不是自己再“发明轮子”，毕竟并发工具类不是随随便便就能发明成功的
2. 迫不得已时才使用低级的同步原语：低级的同步原语主要指的是 synchronized、Lock、Semaphore 等，这些虽然感觉简单，但实际上并没那么简单，一定要小心使用
3. 避免过早优化：安全第一，并发程序首先要保证安全，出现性能瓶颈后再优化。在设计期和开发期，很多人经常会情不自禁地预估性能的瓶颈，并对此实施优化，但残酷的现实却是：性能瓶颈不是你想预估就能预估的

# 锁的设计

## 用锁的最佳实践

> 锁，应是私有的、不可变的、不可重用的

- 一个是锁有可能会变化，另一个是 Integer 和 String 类型的对象不适合做锁。如果锁发生变化，就意味着失去了互斥功能。 Integer 和 String 类型的对象在 JVM 里面是可能被重用的，除此之外，JVM 里可能被重用的对象还有 Boolean，那重用意味着什么呢？意味着你的锁可能被其他代码使用，如果其他代码 `synchronized(你的锁)`，而且不释放，那你的程序就永远拿不到锁，这是隐藏的风险

- 最佳实践（加上final关键字）

  ```
  // 普通对象锁
  private final Object 
    lock = new Object();
  // 静态对象锁
  private static final Object
    lock = new Object(); 
  ```

- 永远只在更新对象的成员变量时加锁
- 永远只在访问可变的成员变量时加锁
- 永远不在调用其他对象的方法时加锁

## 锁的性能要看场景

## 竞态条件需要格外关注

## 方法调用时先计算参数

- 方法的调用，是先计算参数，然后将参数压入调用栈之后才会执行方法体，如`set(get()+1)`
- 日志：`logger.debug("The var1：" + var1 + ", var2:" + var2);`这样会导致计算，即使日志级别不对应。所以需要使用占位符，`logger.debug("The var1：{}, var2:{}",var1, var2);`

## InterruptedException 异常处理需小心

- 当你调用 Java 对象的 wait() 方法或者线程的 sleep() 方法时，需要捕获并处理 InterruptedException 异常
- 在触发 InterruptedException 异常的同时，JVM 会同时把线程的中断标志位清除，一般在捕获异常之后，重新设置中断标记位

# Lock

## 再造管程的原因

1. **能够响应中断**。synchronized 的问题是，持有锁 A 后，如果尝试获取锁 B 失败，那么线程就进入阻塞状态，一旦发生死锁，就没有任何机会来唤醒阻塞的线程。但如果阻塞状态的线程能够响应中断信号，也就是说当我们给阻塞的线程发送中断信号的时候，能够唤醒它，那它就有机会释放曾经持有的锁 A。这样就破坏了不可抢占条件了。
2. **支持超时**。如果线程在一段时间之内没有获取到锁，不是进入阻塞状态，而是返回一个错误，那这个线程也有机会释放曾经持有的锁。这样也能破坏不可抢占条件。
3. **非阻塞地获取锁**。如果尝试获取锁失败，并不进入阻塞状态，而是直接返回，那这个线程也有机会释放曾经持有的锁。这样也能破坏不可抢占条件

## 可重入锁

- **所谓可重入锁，顾名思义，指的是线程可以重复获取同一把锁**
- **可重入函数，指的是多个线程可以同时调用该函数**，每个线程都能得到正确结果；同时在一个线程内支持线程切换，无论被切换多少次，结果都是正确的

## 公平锁与非公平锁

> 在使用 ReentrantLock 的时候，你会发现 ReentrantLock 这个类有两个构造函数，一个是无参构造函数，一个是传入 fair 参数的构造函数。fair 参数代表的是锁的公平策略，如果传入 true 就表示需要构造一个公平锁，反之则表示要构造一个非公平锁

# Condition

> **Condition 实现了管程模型里面的条件变量**

- 一个阻塞队列，需要两个条件变量，一个是队列不空（空队列不允许出队），另一个是队列不满（队列已满不允许入队）
- Lock 和 Condition 实现的管程，**线程等待和通知需要调用 await()、signal()、signalAll()**，它们的语义和 wait()、notify()、notifyAll() 是相同的。但是不一样的是，Lock&Condition 实现的管程里只能使用前面的 await()、signal()、signalAll()，而后面的 wait()、notify()、notifyAll() 只有在 synchronized 实现的管程里才能使用。如果一不小心在 Lock&Condition 实现的管程里调用了 wait()、notify()、notifyAll()，那程序可就彻底玩儿完了

# 异步与同步

- 同步是 Java 代码默认的处理方式。如果你想让你的程序支持异步，可以通过下面两种方式来实现：
  1. 调用方创建一个子线程，在子线程中执行方法调用，这种调用我们称为异步调用；
  2. 方法实现的时候，创建一个新的线程执行主要逻辑，主线程直接 return，这种方法我们一般称为异步方法

- 其实在编程领域，异步的场景还是挺多的，比如 TCP 协议本身就是异步的，我们工作中经常用到的 RPC 调用，**在 TCP 协议层面，发送完 RPC 请求后，线程是不会等待 RPC 的响应结果的**。但是平时工作中的 RPC 调用大多数都是同步的，这是因为有框架做了异步转同步的事情。例如目前知名的 RPC 框架 Dubbo 就给我们做了异步转同步的事情，Dubbo 异步转同步的功能应该是通过 DefaultFuture 这个类实现的（等待通知机制）

# Semaphore

## 模型

- 信号量模型还是很简单的，可以简单概括为：**一个计数器，一个等待队列，三个方法**。在信号量模型里，计数器和等待队列对外是透明的，所以只能通过信号量模型提供的三个方法来访问它们，这三个方法分别是：init()、down() 和 up()
  - init()：设置计数器的初始值。
  - down()：计数器的值减 1；如果此时计数器的值小于 0，则当前线程将被阻塞，否则当前线程可以继续执行。
  - up()：计数器的值加 1；如果此时计数器的值小于或者等于 0，则唤醒等待队列中的一个线程，并将其从等待队列中移除
- 信号量模型里面，down()、up() 这两个操作历史上最早称为 P 操作和 V 操作，所以信号量模型也被称为**PV原语**。另外，还有些人喜欢用 semWait() 和 semSignal() 来称呼它们，虽然叫法不同，但是语义都是相同的。在 Java SDK 并发包里，down() 和 up() 对应的则是 acquire() 和 release()

## 限流

> 除了实现一个互斥锁，Semaphore 还有一个功能是 Lock 不容易实现的，那就是**Semaphore 可以允许多个线程访问一个临界区**

- 限制进入临界区的线程数目，但是进入临界区的线程仍然需要同步

# ReadWriteLock

> 针对读多写少这种并发场景，Java SDK 并发包提供了读写锁——ReadWriteLock，非常容易使用，并且性能很好

## 读写锁原则

- 允许多个线程同时读共享变量；
- 只允许一个线程写共享变量；
- 如果一个写线程正在执行写操作，此时禁止读线程读共享变量。

## 读写锁实现缓存

- ReadWriteLock是一个接口，它的实现类是 `ReentrantReadWriteLock`，通过名字你应该就能判断出来，它是支持可重入的

- 在获取写锁之后，我们并没有直接去查询数据库，而是重新验证了一次缓存中是否存在，再次验证如果还是不存在，我们才去查询数据库并更新本地缓存。再次验证的方式，能够避免高并发场景下重复查询数据的问题
- 只有写锁支持条件变量，读锁是不支持条件变量的，读锁调用 newCondition() 会抛出 UnsupportedOperationException 异常
- 数据一致性问题：数据同步指的是保证缓存数据和源头数据的一致性。
  - **超时机制**：所谓超时机制指的是加载进缓存的数据不是长久有效的，而是有时效的，当缓存的数据超过时效，也就是超时之后，这条数据在缓存中就失效了。而访问缓存中失效的数据，会触发缓存重新从源头把数据加载进缓存
  - 当然也可以在源头数据发生变化时，快速反馈给缓存，但这个就要依赖具体的场景了。例如 MySQL 作为数据源头，可以通过近实时地解析 binlog 来识别数据是否发生了变化，如果发生了变化就将最新的数据**推送**给缓存
  - 另外，还有一些方案采取的是数据库和缓存的**双写**方案。

## 读写锁的升级与降级

- 虽然锁的升级是不允许的，但是锁的降级却是允许的

# StampedLock

> 在读多写少的场景中，Java 在 1.8 这个版本里，提供了一种叫 StampedLock 的锁，它的性能比读写锁还要好（Stamped，铭刻的；盖上邮戳的）

- ReadWriteLock 支持两种模式：一种是读锁，一种是写锁。而 StampedLock 支持三种模式，分别是：**写锁**、**悲观读锁**和**乐观读**。其中，写锁、悲观读锁的语义和 ReadWriteLock 的写锁、读锁的语义非常类似，允许多个线程同时获取悲观读锁，但是只允许一个线程获取写锁，写锁和悲观读锁是互斥的。不同的是：StampedLock 里的写锁和悲观读锁加锁成功之后，都会返回一个 stamp；然后解锁的时候，需要传入这个 stamp

- StampedLock 的性能之所以比 ReadWriteLock 还要好，其关键是 StampedLock 支持乐观读的方式。ReadWriteLock 支持多个线程同时读，但是当多个线程同时读的时候，所有的写操作会被阻塞；而 StampedLock 提供的乐观读，是允许一个线程获取写锁的，也就是说不是所有的写操作都被阻塞。**乐观读这个操作是无锁的**，所以相比较 ReadWriteLock 的读锁，乐观读的性能更好一些
- 数据库里的乐观锁，查询的时候需要把 version 字段查出来，更新的时候要利用 version 字段做验证。这个 version 字段就类似于 StampedLock 里面的 stamp

- **StampedLock 不支持重入**

- StampedLock 的悲观读锁、写锁都不支持条件变量

- 如果线程阻塞在 StampedLock 的 readLock() 或者 writeLock() 上时，此时调用该阻塞线程的 interrupt() 方法，会导致 CPU 飙升。所以，**使用 StampedLock 一定不要调用中断操作，如果需要支持中断功能，一定使用可中断的悲观读锁 readLockInterruptibly() 和写锁 writeLockInterruptibly()**

- 模板代码

  ```
  // read 
  final StampedLock sl = 
    new StampedLock();
   
  // 乐观读
  long stamp = 
    sl.tryOptimisticRead();
  // 读入方法局部变量
  ......
  // 校验 stamp，使用validate方法
  if (!sl.validate(stamp)){
    // 升级为悲观读锁
    stamp = sl.readLock();
    try {
      // 读入方法局部变量
      .....
    } finally {
      // 释放悲观读锁
      sl.unlockRead(stamp);
    }
  }
  // 使用方法局部变量执行业务操作
  ......
  
  // write
  long stamp = sl.writeLock();
  try {
    // 写共享变量
    ......
  } finally {
    sl.unlockWrite(stamp);
  }
  ```

- StampedLock 支持锁的降级（通过 tryConvertToReadLock() 方法实现）和升级（通过 tryConvertToWriteLock() 方法实现）

# CountDownLatch

- 计数器

# CyclicBarrier

- CyclicBarrier 的计数器有自动重置的功能，当减到 0 的时候，会自动重置你设置的初始值

- **CountDownLatch 主要用来解决一个线程等待多个线程的场景**，可以类比旅游团团长要等待所有的游客到齐才能去下一个景点；而**CyclicBarrier 是一组线程之间互相等待**，更像是几个驴友之间不离不弃。除此之外 CountDownLatch 的计数器是不能循环利用的，也就是说一旦计数器减到 0，再有线程调用 await()，该线程会直接通过。但**CyclicBarrier 的计数器是可以循环利用的**，而且具备自动重置的功能，一旦计数器减到 0 会自动重置到你设置的初始值。除此之外，CyclicBarrier 还可以设置回调函数，可以说是功能丰富。

# 并发容器

> Java 中的容器主要可以分为四个大类，分别是 List、Map、Set 和 Queue，但并不是所有的 Java 容器都是线程安全的

## 同步容器

> Java 1.5 之前提供的**同步容器**虽然也能保证线程安全，但是性能很差

- JDK在 Collections 这个类中还提供了一套完备的包装类，比如下面的示例代码中，分别把 ArrayList、HashSet 和 HashMap 包装成了线程安全的 List、Set 和 Map
- 经过包装后线程安全容器，都是基于 synchronized 这个同步关键字实现的，所以也被称为**同步容器**。Java 提供的同步容器还有 Vector、Stack 和 Hashtable，这三个容器不是基于包装类实现的，但同样是基于 synchronized 实现的，对这三个容器的遍历，同样要加锁保证互斥

## 并发容器

### List

- List 里面只有一个实现类就是**CopyOnWriteArrayList**。CopyOnWrite，顾名思义就是写的时候会将共享变量新复制一份出来，这样做的好处是读操作完全无锁。
- CopyOnWriteArrayList 内部维护了一个数组，成员变量 array 就指向这个内部数组，所有的读操作都是基于 array 进行的，如下图所示，迭代器 Iterator 遍历的就是 array 数组
- CopyOnWriteArrayList 会将 array 复制一份，然后在新复制处理的数组上执行增加元素的操作，执行完之后再将 array 指向这个新的数组。读写是可以并行的，遍历操作一直都是基于原 array 执行，而写操作则是基于新 array 进行
- CopyOnWriteArrayList 仅适用于写操作非常少的场景，而且能够容忍读写的短暂不一致。例如上面的例子中，写入的新元素并不能立刻被遍历到
- CopyOnWriteArrayList 迭代器是只读的，不支持增删改。因为迭代器遍历的仅仅是一个快照，而对快照进行增删改是没有意义的

### Map

- Map 接口的两个实现是 ConcurrentHashMap 和 ConcurrentSkipListMap，它们从应用的角度来看，主要区别在于**ConcurrentHashMap 的 key 是无序的，而 ConcurrentSkipListMap 的 key 是有序的**。所以如果你需要保证 key 的顺序，就只能使用 ConcurrentSkipListMap

- 使用 ConcurrentHashMap 和 ConcurrentSkipListMap 需要注意的地方是，它们的 key 和 value 都不能为空，否则会抛出`NullPointerException`这个运行时异常

### Set

- Set 接口的两个实现是 CopyOnWriteArraySet 和 ConcurrentSkipListSet

### Queue

- 可以从以下两个维度来分类。一个维度是**阻塞与非阻塞**，所谓阻塞指的是当队列已满时，入队操作阻塞；当队列已空时，出队操作阻塞。另一个维度是**单端与双端**，单端指的是只能队尾入队，队首出队；而双端指的是队首队尾皆可入队出队。Java 并发包里**阻塞队列都用 Blocking 关键字标识，单端队列使用 Queue 标识，双端队列使用 Deque 标识**
- **单端阻塞队列**：其实现有 ArrayBlockingQueue、LinkedBlockingQueue、SynchronousQueue、LinkedTransferQueue、PriorityBlockingQueue 和 DelayQueue。内部一般会持有一个队列，这个队列可以是数组（其实现是 ArrayBlockingQueue）也可以是链表（其实现是 LinkedBlockingQueue）；甚至还可以不持有队列（其实现是 SynchronousQueue），此时生产者线程的入队操作必须等待消费者线程的出队操作。而 LinkedTransferQueue 融合 LinkedBlockingQueue 和 SynchronousQueue 的功能，性能比 LinkedBlockingQueue 更好；PriorityBlockingQueue 支持按照优先级出队；DelayQueue 支持延时出队
- **双端阻塞队列**：其实现是 LinkedBlockingDeque
- **单端非阻塞队列**：其实现是 ConcurrentLinkedQueue
- **双端非阻塞队列**：其实现是 ConcurrentLinkedDeque
- 使用队列时，需要格外注意队列是否支持有界（所谓有界指的是内部的队列是否有容量限制）。实际工作中，一般都不建议使用无界的队列，因为数据量大了之后很容易导致 OOM。上面我们提到的这些 Queue 中，只有 ArrayBlockingQueue 和 LinkedBlockingQueue 是支持有界的，所以**在使用其他无界队列时，一定要充分考虑是否存在导致 OOM 的隐患**

## 快速失败与安全失败

> HashMap、ArrayList 这些集合类，这些在 java.util 包的集合类就都是快速失败的；而 java.util.concurrent 包下的类都是安全失败
>
> 当你选对容器的时候，根本不会触发 Java 容器的快速失败机制（Fail-Fast）

- 快速失败：在使用迭代器对集合对象进行遍历的时候，如果 A 线程正在对集合进行遍历，此时 B 线程对集合进行**修改**（增加、删除、修改），或者 A 线程在遍历过程中对集合进行修改，都会导致 A 线程抛出 ConcurrentModificationException （并发修改）异常。原因是迭代器在遍历时直接访问集合中的内容，并且在遍历过程中使用一个 modCount 变量。集合在被遍历期间如果内容发生变化，就会改变 modCount 的值。每当迭代器使用 hashNext()/next() 遍历下一个元素之前，都会检测 modCount 变量是否为 expectedModCount 值，是的话就返回遍历；否则抛出异常，终止遍历
- 安全失败：采用安全失败机制的集合容器，在遍历时不是直接在集合内容上访问的，而是先复制原有集合内容，在拷贝的集合上进行遍历。由于迭代时是对原集合的拷贝进行遍历，所以在遍历过程中对原集合所作的修改并不能被迭代器检测到，故不会抛 ConcurrentModificationException 异常

## 容器的遍历

- 通过迭代器遍历容器，对每个元素调用方法，这有可能存在并发问题，因为这些组合的操作不具备原子性

  ```
  List list = Collections.synchronizedList(new ArrayList());
  Iterator i = list.iterator(); 
  while (i.hasNext()){
  	foo(i.next());
  }
  ```

- 而正确做法是锁住容器之后再执行遍历操作。如果你查看 Collections 内部的包装类源码，你会发现包装类的公共方法锁的是对象的 this，其实就是我们这里的 list，所以锁住 list 绝对是线程安全的

  ```
  List list = Collections.synchronizedList(new ArrayList());
  synchronized (list) {  
    Iterator i = list.iterator(); 
    while (i.hasNext()){
    	foo(i.next());
    }
  }    
  ```

# CAS

> CAS有可能导致活锁与饥饿问题

- CPU 为了解决并发问题，提供了 CAS 指令（CAS，全称是 Compare And Swap，即“比较并交换”）。CAS 指令包含 3 个参数：共享变量的内存地址 A、用于比较的值 B 和共享变量的新值 C；并且只有当内存中地址 A 处的值等于 B 时，才能将内存中地址 A 处的值更新为新值 C。**作为一条 CPU 指令，CAS 指令本身是能够保证原子性的**

  ```
  class SimulatedCAS{
    int count；
    synchronized int cas(
      int expect, int newValue){
      // 读目前 count 的值
      int curValue = count;
      // 比较目前 count 值是否 == 期望值
      if(curValue == expect){
        // 如果是，则更新 count 的值
        count = newValue;
      }
      // 返回写入前的值
      return curValue;
    }
  }
  ```

- 使用 CAS 来解决并发问题，一般都会伴随着自旋，而所谓自旋，其实就是循环尝试。如果读取的值不等于预期值，可以重新读 count 最新的值来计算 newValue 并尝试再次更新，直到成功

  ```
  class SimulatedCAS{
    volatile int count;
    // 实现 count+=1
    addOne(){
      do {
        newValue = count+1; //①
      }while(count !=
        cas(count,newValue) //②
    }
    // 模拟实现 CAS，仅用来帮助理解
    synchronized int cas(
      int expect, int newValue){
      // 读目前 count 的值
      int curValue = count;
      // 比较目前 count 值是否 == 期望值
      if(curValue == expect){
        // 如果是，则更新 count 的值
        count= newValue;
      }
      // 返回写入前的值
      return curValue;
    }
  }
  ```



# 原子类

> 累加器中，自增方法不是线程安全的，问题就出在变量 count 的可见性和 count+=1 的原子性上。可见性问题可以用 volatile 来解决，而原子性问题我们前面一直都是采用的互斥锁方案。其实对于简单的原子性问题，还有一种**无锁方案**。JDK 并发包将这种无锁方案封装提炼之后，实现了一系列的原子类

## 实现

- getAndIncrement() 方法会转调 unsafe.getAndAddLong() 方法。这里 this 和 valueOffset 两个参数可以唯一确定共享变量的内存地址

  ```
  final long getAndIncrement() {
    return unsafe.getAndAddLong(
      this, valueOffset, 1L);
  }
  ```

## 基本数据类型

> 相关实现有 AtomicBoolean、AtomicInteger 和 AtomicLong

```
getAndIncrement() // 原子化 i++
getAndDecrement() // 原子化的 i--
incrementAndGet() // 原子化的 ++i
decrementAndGet() // 原子化的 --i
// 当前值 +=delta，返回 += 前的值
getAndAdd(delta) 
// 当前值 +=delta，返回 += 后的值
addAndGet(delta)
//CAS 操作，返回是否成功
compareAndSet(expect, update)
// 以下四个方法
// 新值可以通过传入 func 函数来计算
getAndUpdate(func)
updateAndGet(func)
getAndAccumulate(x,func)
accumulateAndGet(x,func)
```

## 原子化的对象引用类型

> 相关实现有 AtomicReference、AtomicStampedReference 和 AtomicMarkableReference，利用它们可以实现对象引用的原子化更新

- 对象引用的更新需要重点关注 ABA 问题，AtomicStampedReference 和 AtomicMarkableReference 这两个原子类可以解决 ABA 问题

## 原子化数组

> 相关实现有 AtomicIntegerArray、AtomicLongArray 和 AtomicReferenceArray，利用这些原子类，我们可以原子化地更新数组里面的每一个元素。这些类提供的方法和原子化的基本数据类型的区别仅仅是：每个方法多了一个数组的索引参数，所以这里也不再赘述了

## 原子化对象属性更新器

> 相关实现有 AtomicIntegerFieldUpdater、AtomicLongFieldUpdater 和 AtomicReferenceFieldUpdater，利用它们可以原子化地更新对象的属性，这三个方法都是利用反射机制实现的

- **对象属性必须是 volatile 类型的，只有这样才能保证可见性**；如果对象属性不是 volatile 类型的，newUpdater() 方法会抛出 IllegalArgumentException 这个运行时异常

## 原子化的累加器

> DoubleAccumulator、DoubleAdder、LongAccumulator 和 LongAdder，这四个类仅仅用来执行累加操作，相比原子化的基本数据类型，速度更快，但是不支持 compareAndSet() 方法。如果你仅仅需要累加操作，使用原子化的累加器性能会更好

# Executor与线程池

> 创建线程远不是创建一个对象那么简单。创建对象，仅仅是在 JVM 的堆里分配一块内存而已；而创建一个线程，却需要调用操作系统内核的 API，然后操作系统要为线程分配一系列的资源，这个成本就很高了，所以**线程是一个重量级的对象，应该避免频繁创建和销毁**

- Java线程池没有采用一般意义上的池化资源的设计模式，即申请归还模式，而是采用了生产者消费者模式，因为Thread不存在类似 execute(Runnable target) 这样的公共方法，无法很好地完成线程的执行动作

## ThreadPoolExecutor

- 构造方法

  ```
  ThreadPoolExecutor
  (
    int corePoolSize, // 线程池保有的最小线程数
    int maximumPoolSize, // 线程池创建的最大线程数
    long keepAliveTime,
    TimeUnit unit, // 工作线程允许的空闲时间
    BlockingQueue<Runnable> workQueue, //工作队列
    ThreadFactory threadFactory, //可以自定义如何创建线程，例如可以给线程指定一个有意义的名字
    RejectedExecutionHandler handler // 拒绝策略
  ) 
  ```

- 拒绝策略
  - CallerRunsPolicy：提交任务的线程自己去执行该任务。
  - AbortPolicy：默认的拒绝策略，会 throws RejectedExecutionException。
  - DiscardPolicy：直接丢弃任务，没有任何异常抛出。
  - DiscardOldestPolicy：丢弃最老的任务，其实就是把最早进入工作队列的任务丢弃，然后把新任务加入到工作队列

- 注意事项

  - 考虑到 ThreadPoolExecutor 的构造函数实在是有些复杂，所以 Java 并发包里提供了一个线程池的静态工厂类 Executors，利用 Executors 你可以快速创建线程池。不过目前大厂的编码规范中基本上都不建议使用 Executors 了，所以这里我就不再花篇幅介绍了。

  - 不建议使用 Executors 的最重要的原因是：Executors 提供的很多方法默认使用的都是无界的 LinkedBlockingQueue，高负载情境下，无界队列很容易导致 OOM，而 OOM 会导致所有请求都无法处理，这是致命问题。所以**强烈建议使用有界队列**。

  - 使用有界队列，当任务过多时，线程池会触发执行拒绝策略，线程池默认的拒绝策略会 throw RejectedExecutionException 这是个运行时异常，对于运行时异常编译器并不强制 catch 它，所以开发人员很容易忽略。因此**默认拒绝策略要慎重使用**。如果线程池处理的任务非常重要，建议自定义自己的拒绝策略；并且在实际工作中，自定义的拒绝策略往往和降级策略配合使用。

  - 使用线程池，还要注意异常处理的问题，例如通过 ThreadPoolExecutor 对象的 execute() 方法提交任务时，如果任务在执行的过程中出现运行时异常，会导致执行任务的线程终止；不过，最致命的是任务虽然异常了，但是你却获取不到任何通知，这会让你误以为任务都执行得很正常。虽然线程池提供了很多用于异常处理的方法，但是最稳妥和简单的方案还是捕获所有异常并按需处理，你可以参考下面的示例代码。

## 获取任务执行结果

> Java 通过 ThreadPoolExecutor 提供的 3 个 submit() 方法和 1 个 FutureTask 工具类来支持获得任务执行结果的需求

- 提交

  ```
  // 提交 Runnable 任务
  Future<?> submit(Runnable task);
  // 提交 Callable 任务
  <T> Future<T> submit(Callable<T> task);
  // 提交 Runnable 任务及结果引用  
  <T> Future<T> submit(Runnable task, T result);
  ```

  1. 提交 Runnable 任务 `submit(Runnable task)` ：这个方法的参数是一个 Runnable 接口，Runnable 接口的 run() 方法是没有返回值的，所以 `submit(Runnable task)` 这个方法返回的 Future 仅可以用来断言任务已经结束了，类似于 Thread.join()。

  2. 提交 Callable 任务 `submit(Callable task)`：这个方法的参数是一个 Callable 接口，它只有一个 call() 方法，并且这个方法是有返回值的，所以这个方法返回的 Future 对象可以通过调用其 get() 方法来获取任务的执行结果。

  3. 提交 Runnable 任务及结果引用 `submit(Runnable task, T result)`：这个方法很有意思，假设这个方法返回的 Future 对象是 f，f.get() 的返回值就是传给 submit() 方法的参数 result。这个方法该怎么用呢？下面这段示例代码展示了它的经典用法。需要你注意的是 Runnable 接口的实现类 Task 声明了一个有参构造函数 `Task(Result r)` ，创建 Task 对象的时候传入了 result 对象，这样就能在类 Task 的 run() 方法中对 result 进行各种操作了。result 相当于主线程和子线程之间的桥梁，通过它主子线程可以共享数据。

     ```
     ExecutorService executor 
       = Executors.newFixedThreadPool(1);
     // 创建 Result 对象 r
     Result r = new Result();
     r.setAAA(a);
     // 提交任务
     Future<Result> future = 
       executor.submit(new Task(r), r);  
     Result fr = future.get();
     // 下面等式成立
     fr === r;
     fr.getAAA() === a;
     fr.getXXX() === x
      
     class Task implements Runnable{
       Result r;
       // 通过构造函数传入 result
       Task(Result r){
         this.r = r;
       }
       void run() {
         // 可以操作 result
         a = r.getAAA();
         r.setXXX(x);
       }
     }
     ```

- Future

  ```
  // 取消任务
  boolean cancel(boolean mayInterruptIfRunning);
  // 判断任务是否已取消  
  boolean isCancelled();
  // 判断任务是否已结束
  boolean isDone();
  // 获得任务执行结果
  get();
  // 获得任务执行结果，支持超时
  get(long timeout, TimeUnit unit);
  ```

- FutureTask

  - 构造方法

  ```
  FutureTask(Callable<V> callable);
  FutureTask(Runnable runnable, V result);
  ```

  - FutureTask 实现了 Runnable 和 Future 接口，由于实现了 Runnable 接口，所以可以将 FutureTask 对象作为任务提交给 ThreadPoolExecutor 去执行，也可以直接被 Thread 执行；又因为实现了 Future 接口，所以也能用来获得任务的执行结果

# CompletableFuture异步编程

> **异步化**，是并行方案得以实施的基础，更深入地讲其实就是：**利用多线程优化性能这个核心方案得以实施的基础**

## 优点

1. 无需手工维护线程，没有繁琐的手工维护线程的工作，给任务分配线程的工作也不需要我们关注；
2. 语义更清晰，例如 `f3 = f1.thenCombine(f2, ()->{})` 能够清晰地表述“任务 3 要等待任务 1 和任务 2 都完成后才能开始”；
3. 代码更简练并且专注于业务逻辑，几乎所有代码都是业务逻辑相关的。

## 创建

- 构造方法

  Runnable 接口的 run() 方法没有返回值，而 Supplier 接口的 get() 方法是有返回值的

  ```
  // 使用默认线程池
  static CompletableFuture<Void> 
    runAsync(Runnable runnable)
    
  static <U> CompletableFuture<U> 
    supplyAsync(Supplier<U> supplier)
    
  // 可以指定线程池  
  static CompletableFuture<Void> 
    runAsync(Runnable runnable, Executor executor)
    
  static <U> CompletableFuture<U> 
    supplyAsync(Supplier<U> supplier, Executor executor) 
  ```

- 前两个方法和后两个方法的区别在于：后两个方法可以指定线程池参数。默认情况下 CompletableFuture 会使用公共的 ForkJoinPool 线程池，这个线程池默认创建的线程数是 CPU 的核数（也可以通过 JVM option:-Djava.util.concurrent.ForkJoinPool.common.parallelism 来设置 ForkJoinPool 线程池的线程数）。如果所有 CompletableFuture 共享一个线程池，那么一旦有任务执行一些很慢的 I/O 操作，就会导致线程池中所有线程都阻塞在 I/O 操作上，从而造成线程饥饿，进而影响整个系统的性能。所以，强烈建议根据不同的业务类型创建不同的线程池，以避免互相干扰
- 创建完 CompletableFuture 对象之后，会自动地异步执行 runnable.run() 方法或者 supplier.get() 方法，对于一个异步操作，你需要关注两个问题：一个是异步操作什么时候结束，另一个是如何获取异步操作的执行结果。因为 CompletableFuture 类实现了 Future 接口，所以这两个问题你都可以通过 Future 接口来解决

## CompletionStage接口

- 任务是有时序关系的，比如有**串行关系、并行关系、汇聚关系**等，CompletionStage 接口可以清晰地描述任务之间的这种时序关系

### 描述串行关系

- CompletionStage 接口里面描述串行关系，主要是 thenApply、thenAccept、thenRun 和 thenCompose 这四个系列的接口

### 描述 AND 汇聚关系

- CompletionStage 接口里面描述 AND 汇聚关系，主要是 thenCombine、thenAcceptBoth 和 runAfterBoth 系列的接口，这些接口的区别也是源自 fn、consumer、action 这三个核心参数不同

### 描述 OR 汇聚关系

- CompletionStage 接口里面描述 OR 汇聚关系，主要是 applyToEither、acceptEither 和 runAfterEither 系列的接口，这些接口的区别也是源自 fn、consumer、action 这三个核心参数不同

### 异常处理

- 使用 exceptionally() 方法来处理异常，exceptionally() 的使用非常类似于 try{}catch{}中的 catch{}，但是由于支持链式编程方式，所以相对更简单。既然有 try{}catch{}，那就一定还有 try{}finally{}，whenComplete() 和 handle() 系列方法就类似于 try{}finally{}中的 finally{}，无论是否发生异常都会执行 whenComplete() 中的回调函数 consumer 和 handle() 中的回调函数 fn。whenComplete() 和 handle() 的区别在于 whenComplete() 不支持返回结果，而 handle() 是支持返回结果的

# CompletionService 批量执行异步任务

> CompletionService 的实现原理也是内部维护了一个阻塞队列，当任务执行结束就把任务的执行结果加入到阻塞队列中，不同的是 CompletionService 是把任务执行结果的 Future 对象加入到阻塞队列中

## 创建

- CompletionService 接口的实现类是 ExecutorCompletionService

  ```
  // 默认使用无界的 LinkedBlockingQueue
  ExecutorCompletionService(Executor executor)；
  ExecutorCompletionService(Executor executor, BlockingQueue<Future<V>> completionQueue)
  ```

## 应用场景

- Dubbo 中有一种叫做**Forking 的集群模式**，这种集群模式下，支持**并行地调用多个查询服务，只要有一个成功返回结果，整个服务就可以返回了**，利用 CompletionService 可以快速实现 Forking 这种集群模式

# Fork/join

> 对于简单的并行任务，你可以通过“线程池 +Future”的方案来解决；如果任务之间有聚合关系，无论是 AND 聚合还是 OR 聚合，都可以通过 CompletableFuture 来解决；而批量的并行任务，则可以通过 CompletionService 来解决。上面提到的简单并行、聚合、批量并行这三种任务模型，基本上能够覆盖日常工作中的并发场景了，但还是不够全面，因为还有一种“分治”的任务模型没有覆盖到。**分治**，顾名思义，即分而治之，是一种解决复杂问题的思维方法和模式；具体来讲，指的是**把一个复杂的问题分解成多个相似的子问题，然后再把子问题分解成更小的子问题，直到子问题简单到可以直接求解**。理论上来讲，解决每一个问题都对应着一个任务，所以对于问题的分治，实际上就是对于任务的分治。

- Fork/Join 是一个并行计算的框架，主要就是用来支持分治任务模型的，这个计算框架里的**Fork 对应的是分治任务模型里的任务分解，Join 对应的是结果合并**。Fork/Join 计算框架主要包含两部分，一部分是**分治任务的线程池 ForkJoinPool**，另一部分是**分治任务 ForkJoinTask**。这两部分的关系类似于 ThreadPoolExecutor 和 Runnable 的关系，都可以理解为提交任务到线程池，只不过分治任务有自己独特类型 ForkJoinTask
- ForkJoinTask 是一个抽象类，它的方法有很多，最核心的是 fork() 方法和 join() 方法，其中 fork() 方法会异步地执行一个子任务，而 join() 方法则会阻塞当前线程来等待子任务的执行结果。ForkJoinTask 有两个子类——RecursiveAction 和 RecursiveTask，通过名字你就应该能知道，它们都是用递归的方式来处理分治任务的。这两个子类都定义了抽象方法 compute()，不过区别是 RecursiveAction 定义的 compute() 没有返回值，而 RecursiveTask 定义的 compute() 方法是有返回值的。这两个子类也是抽象类，在使用的时候，需要你定义子类去扩展
- **任务窃取**算法：ForkJoinPool 中的任务队列采用的是双端队列，工作线程正常获取任务和“窃取任务”分别是从任务队列不同的端消费，这样能避免很多不必要的数据竞争

- Fork/Join 并行计算框架的核心组件是 ForkJoinPool。ForkJoinPool 支持任务窃取机制，能够让所有线程的工作量基本均衡，不会出现有的线程很忙，而有的线程很闲的状况，所以性能很好。Java 1.8 提供的 Stream API 里面并行流也是以 ForkJoinPool 为基础的。不过需要你注意的是，默认情况下所有的并行流计算都共享一个 ForkJoinPool，这个共享的 ForkJoinPool 默认的线程数是 CPU 的核数；如果所有的并行流计算都是 CPU 密集型计算的话，完全没有问题，但是如果存在 I/O 密集型的并行流计算，那么很可能会因为一个很慢的 I/O 计算而拖慢整个系统的性能。所以**建议用不同的 ForkJoinPool 执行不同类型的计算任务**

------

# Immutability模式

> 解决并发问题，其实最简单的办法就是让共享变量只有读操作，而没有写操作。这个办法如此重要，以至于被上升到了一种解决并发问题的设计模式：**不变性（Immutability）模式**。所谓**不变性，简单来讲，就是对象一旦被创建之后，状态就不再发生变化**

## 创建具备不可变性的类

- **将一个类所有的属性都设置成 final 的，并且只允许存在只读方法，那么这个类基本上就具备不可变性了**。更严格的做法是**这个类本身也是 final 的**，也就是不允许继承。因为子类可以覆盖父类的方法，有可能改变不可变性，所以推荐你在实际工作中，使用这种更严格的做法
- JDK 里很多类都具备不可变性，只是由于它们的使用太简单，最后反而被忽略了。例如经常用到的 String（String 这个类以及它的属性 value[] 都是 final 的；而 replace() 方法的实现，没有修改 value[]，而是将替换后的字符串作为返回值返回了）和 Long、Integer、Double 等基础类型的包装类都具备不可变性，这些对象的线程安全性都是靠不可变性来保证的
- 如果具备不可变性的类，需要提供类似修改的功能，具体该怎么操作呢？做法很简单，那就是**创建一个新的不可变对象**，这是与可变对象的一个重要区别，可变对象往往是修改自己的属性。所有的修改操作都创建一个新的不可变对象，的确有些浪费内存

## 利用享元模式避免创建重复对象

> 利用享元模式可以减少创建对象的数量，从而减少内存占用

- 享元模式本质上其实就是一个**对象池**，利用享元模式创建对象的逻辑也很简单：创建之前，首先去对象池里看看是不是存在；如果已经存在，就利用对象池里的对象；如果不存在，就会新创建一个对象，并且把这个新创建出来的对象放进对象池里
- 所有的基础类型的包装类都不适合做锁，因为它们内部用到了享元模式，这会导致看上去私有的锁，其实是共有的

## 注意事项

- 对象的所有属性都是 final 的，并不能保证不可变性；
- 不可变对象也需要正确发布
- 不可变对象虽然是线程安全的，但是并不意味着引用这些不可变对象的对象就是线程安全的
- 具备不变性的对象，只有一种状态，这个状态由对象内部所有的不变属性共同决定。其实还有一种更简单的不变性对象，那就是**无状态**。无状态对象内部没有属性，只有方法。除了无状态的对象，你可能还听说过无状态的服务、无状态的协议等等。无状态有很多好处，最核心的一点就是性能。在多线程领域，无状态对象没有线程安全问题，无需同步处理，自然性能很好；在分布式领域，无状态意味着可以无限地水平扩展，所以分布式领域里面性能的瓶颈一定不是出在无状态的服务节点上

# COW机制

> Copy-on-Write，经常被缩写为 COW 或者 CoW，顾名思义就是**写时复制**

- 本质上来讲，父子进程的地址空间以及数据都是要隔离的，使用 Copy-on-Write 更多地体现的是一种**延时策略，只有在真正需要复制的时候才复制，而不是提前复制好**，同时 Copy-on-Write 还支持按需复制，所以 Copy-on-Write 在操作系统领域是能够提升性能的
- **Copy-on-Write 最大的应用领域在函数式编程领域**。函数式编程的基础是不可变性（Immutability），所以函数式编程里面所有的修改操作都需要 Copy-on-Write 来解决

# 线程本地储存模式

> **线程封闭**，其本质上就是避免共享。可以通过局部变量可以做到避免共享，利用Java 语言提供的线程本地存储（ThreadLocal）也能够做到

## ThreadLocal

- `withInitial`： 用于创建一个线程局部变量，变量的初始化值通过调用Supplier的get方法来确定
- `set`、`get`、`remove`

- 应用场景

  ```
  SimpleDateFormat线程安全化
  
  static class SafeDateFormat {
    // 定义 ThreadLocal 变量
    static final ThreadLocal<DateFormat>
    tl=ThreadLocal.withInitial(
      ()-> new SimpleDateFormat(
        "yyyy-MM-dd HH:mm:ss"));
        
    static DateFormat get(){
      return tl.get();
    }
  }
  // 不同线程执行下面代码
  // 返回的 df 是不同的
  DateFormat df =
    SafeDateFormat.get()；
  ```

## 实现

- Thread 这个类内部有一个私有属性 threadLocals，其类型就是 ThreadLocalMap，ThreadLocalMap 的 Key 是 ThreadLocal
- 

## ThreadLocal 与内存泄露

- 在线程池中使用 ThreadLocal 为什么可能导致内存泄露呢？原因就出在线程池中线程的存活时间太长，往往都是和程序同生共死的，这就意味着 Thread 持有的 ThreadLocalMap 一直都不会被回收，再加上 ThreadLocalMap 中的 Entry 对 ThreadLocal 是弱引用（WeakReference），所以只要 ThreadLocal 结束了自己的生命周期是可以被回收掉的。但是 Entry 中的 Value 却是被 Entry 强引用的，所以即便 Value 的生命周期结束了，Value 也是无法被回收的，从而导致内存泄露。所以需要手动释放

## InheritableThreadLocal 与继承性

- 通过 ThreadLocal 创建的线程变量，其子线程是无法继承的。也就是说你在线程中通过 ThreadLocal 创建了线程变量 V，而后该线程创建了子线程，你在子线程中是无法通过 ThreadLocal 来访问父线程的线程变量 V 的。Java 提供了 InheritableThreadLocal 来支持子线程继承父线程的线程变量这种特性，InheritableThreadLocal 是 ThreadLocal 子类（但是不推荐使用）

# Guarded Suspension模式

> 所谓 Guarded Suspension，直译过来就是“保护性地暂停”，等待唤醒机制的规范实现

# Balking模式

> Balking 模式本质上是一种规范化地解决“多线程版本的 if”的方案

- Balking 模式和 Guarded Suspension 模式从实现上看似乎没有多大的关系，Balking 模式只需要用互斥锁就能解决，而 Guarded Suspension 模式则要用到管程这种高级的并发原语；但是从应用的角度来看，它们解决的都是“线程安全的 if”语义，不同之处在于，Guarded Suspension 模式会等待 if 条件为真，而 Balking 模式不会等待

# Thread-Per-Message模式

> 为每个任务分配一个独立的线程

- Thread-Per-Message 模式的一个最经典的应用场景是**网络编程里服务端的实现**，服务端为每个客户端请求创建一个独立的线程，当线程处理完请求后，自动销毁，这是一种最简单的并发处理网络请求的方法
- Thread-Per-Message 模式作为一种最简单的分工方案，Java 语言支持不了，显然是 Java 语言本身的问题。Java 语言里，Java 线程是和操作系统线程一一对应的，这种做法本质上是将 Java 线程的调度权完全委托给操作系统，而操作系统在这方面非常成熟，所以这种做法的好处是稳定、可靠，但是也继承了操作系统线程的缺点：创建成本高。为了解决这个缺点，Java 并发包里提供了线程池等工具类。这个思路在很长一段时间里都是很稳妥的方案，但是这个方案并不是唯一的方案
- 还有另外一种方案，叫做**轻量级线程**，OpenJDK 有个 Loom 项目，就是要解决 Java 语言的轻量级线程问题，在这个项目中，轻量级线程被叫做**Fiber**
- 只需要将`new Thread(()->{…}).start() 换成 Fiber.schedule(()->{}) `即可

# Worker-Thread模式

> 要想有效避免线程的频繁创建、销毁以及 OOM 问题，就不得不提今天我们要细聊的，也是 Java 领域使用最多的 Worker Thread 模式

# 终止线程

> **两阶段终止模式**。顾名思义，就是将终止过程分成两个阶段，其中第一个阶段主要是线程 T1 向线程 T2**发送终止指令**，而第二阶段则是线程 T2**响应终止指令**

## 两阶段终止模式

- Java 线程进入终止状态的前提是线程进入 RUNNABLE 状态，而实际上线程也可能处在休眠状态，也就是说，我们要想终止一个线程，首先要把线程的状态从休眠状态转换到 RUNNABLE 状态。如何做到呢？这个要靠 Java Thread 类提供的**interrupt() 方法**，它可以将休眠状态的线程转换到 RUNNABLE 状态
- 我们在捕获 Thread.sleep() 的中断异常之后，通过 `Thread.currentThread().interrupt()` 重新设置了线程的中断状态，因为 JVM 的异常处理会清除线程的中断状态
- 我们很可能在线程的 run() 方法中调用第三方类库提供的方法，而我们没有办法保证第三方类库正确处理了线程的中断异常，例如第三方类库在捕获到 Thread.sleep() 方法抛出的中断异常后，没有重新设置线程的中断状态，那么就会导致线程不能够正常终止。所以强烈建议你**设置自己的线程终止标志位**，例如在下面的代码中，使用 isTerminated 作为线程终止标志位，此时无论是否正确处理了线程的中断异常，都不会影响线程优雅地终止（线程终止标志位 isTerminated 需要被声明为 volatile）

## 关闭线程池

- shutdown() 方法是一种很保守的关闭线程池的方法。线程池执行 shutdown() 后，就会拒绝接收新的任务，但是会等待线程池中正在执行的任务和已经进入阻塞队列的任务都执行完之后才最终关闭线程池。
- shutdownNow() 方法，相对就激进一些了，线程池执行 shutdownNow() 后，会拒绝接收新的任务，同时还会中断线程池中正在执行的任务，已经进入阻塞队列的任务也被剥夺了执行的机会，不过这些被剥夺执行机会的任务会作为 shutdownNow() 方法的返回值返回。因为 shutdownNow() 方法会中断正在执行的线程，所以提交到线程池的任务，如果需要优雅地结束，就需要正确地处理线程中断
- 如果提交到线程池的任务不允许取消，那就不能使用 shutdownNow() 方法终止线程池。不过，如果提交到线程池的任务允许后续以补偿的方式重新执行，也是可以使用 shutdownNow() 方法终止线程池的

# 生产者-消费者模式

> Worker Thread 模式类比的是工厂里车间工人的工作模式。但其实在现实世界，工厂里还有一种流水线的工作模式，类比到编程领域，就是**生产者 - 消费者模式**

## 优点

- 生产者 - 消费者模式的核心是一个**任务队列**，生产者线程生产任务，并将任务添加到任务队列中，而消费者线程从任务队列中获取任务并执行
- 生产者 - 消费者模式有一个很重要的优点，就是**解耦**。解耦对于大型系统的设计非常重要，而解耦的一个关键就是组件之间的依赖关系和通信方式必须受限。在生产者 - 消费者模式中，生产者和消费者没有任何依赖关系，它们彼此之间的通信只能通过任务队列，所以**生产者 - 消费者模式是一个不错的解耦方案**
- 生产者 - 消费者模式还有一个重要的优点就是**支持异步，并且能够平衡生产者和消费者的速度差异**。在生产者 - 消费者模式中，生产者线程只需要将任务添加到任务队列而无需等待任务被消费者线程执行完，也就是说任务的生产和消费是异步的，这是与传统的方法之间调用的本质区别，传统的方法之间调用是同步的

## 应用

- 批量插入：注：从任务队列中获取批量任务的方法 pollTasks() 中，首先是以阻塞方式获取任务队列中的一条任务，而后则是以非阻塞的方式获取任务；之所以首先采用阻塞方式，是因为如果任务队列中没有任务，这样的方式能够避免无谓的循环

- 分阶段提交：日志刷盘

# 设计模式总结

## 避免共享的设计模式

- **Immutability 模式**、**Copy-on-Write 模式**和**线程本地存储模式**本质上都是**为了避免共享**，只是实现手段不同而已

## 多线程版本 IF 的设计模式

- **Guarded Suspension 模式**和**Balking 模式**都可以简单地理解为“多线程版本的 if”，但它们的区别在于前者会等待 if 条件变为真，而后者则不需要等待
- Guarded Suspension 模式的经典实现是使用**管程**，很多初学者会简单地用线程 sleep 的方式实现。但不推荐你使用这种方式，最重要的原因是性能，如果 sleep 的时间太长，会影响响应时间；sleep 的时间太短，会导致线程频繁地被唤醒，消耗系统资源
- 实现 Balking 模式最容易忽视的就是**竞态条件问题**

## 分工模式

- **Thread-Per-Message 模式**、**Worker Thread 模式**和**生产者 - 消费者模式**是三种**最简单实用的多线程分工方法**
- Thread-Per-Message 模式在实现的时候需要注意是否存在线程的频繁创建、销毁以及是否可能导致 OOM
- Worker Thread 模式的实现，需要注意潜在的线程**死锁问题**。共享线程池虽然能够提供线程池的使用效率，但一定要保证一个前提，那就是：**任务之间没有依赖关系**

## 终止线程

- 两阶段终止模式是一种通用的解决方案。但其实终止生产者 - 消费者服务还有一种更简单的方案，叫做**“毒丸”**对象。“毒丸”对象是生产者生产的一条特殊任务，然后当消费者线程读到“毒丸”对象时，会立即终止自身的执行

# RateLimiter限流器

> Guava 是 Google 开源的 Java 类库，提供了一个工具类 RateLimiter，解决高并发场景下的限流问题，其在向线程池提交任务之前，调用 `acquire()` 方法就能起到限流的作用

## 令牌桶算法

> **核心是要想通过限流器，必须拿到令牌**。也就是说，只要我们能够限制发放令牌的速率，那么就能控制流速了

- 算法描述
  - 令牌以固定的速率添加到令牌桶中，假设限流的速率是 r/ 秒，则令牌每 1/r 秒会添加一个；
  - 假设令牌桶的容量是 b ，如果令牌桶已满，则新的令牌会被丢弃；（b 其实是 burst 的简写，意义是**限流器允许的最大突发流量**）
  - 请求能够通过限流器的前提是令牌桶中有令牌

- Guava没有使用定时器，其关键是**记录并动态计算下一令牌发放的时间**

## 漏桶算法

# Netty网络模型

## Reactor模式

## Netty的线程模型

- **Netty 中最核心的概念是事件循环（EventLoop）**，其实也就是 Reactor 模式中的 Reactor，**负责监听网络事件并调用事件处理器进行处理**。在 4.x 版本的 Netty 中，网络连接和 EventLoop 是稳定的多对 1 关系，而 EventLoop 和 Java 线程是 1 对 1 关系，这里的稳定指的是关系一旦确定就不再发生变化。也就是说一个网络连接只会对应唯一的一个 EventLoop，而一个 EventLoop 也只会对应到一个 Java 线程，所以**一个网络连接只会对应到一个 Java 线程**
- Netty 中还有一个核心概念是**EventLoopGroup**，顾名思义，一个 EventLoopGroup 由一组 EventLoop 组成。实际使用中，一般都会创建两个 EventLoopGroup，一个称为 bossGroup，一个称为 workerGroup。这个和 socket 处理网络请求的机制有关，socket 处理 TCP 网络连接请求，是在一个独立的 socket 中，每当有一个 TCP 连接成功建立，都会创建一个新的 socket，之后对 TCP 连接的读写都是由新创建处理的 socket 完成的。也就是说**处理 TCP 连接请求和读写请求是通过两个不同的 socket 完成的**
- **在 Netty 中，bossGroup 就用来处理连接请求的，而 workerGroup 是用来处理读写请求的**。bossGroup 处理完连接请求后，会将这个连接提交给 workerGroup 来处理， workerGroup 里面有多个 EventLoop，那新的连接会交给哪个 EventLoop 来处理呢？这就需要一个负载均衡算法，Netty 中目前使用的是**轮询算法**。

- 注意事项
  - 如果 NettybossGroup 只监听一个端口，那 bossGroup 只需要 1 个 EventLoop 就可以了，多了纯属浪费。
  - 默认情况下，Netty 会创建“2*CPU 核数”个 EventLoop，由于网络连接与 EventLoop 有稳定的关系，所以事件处理器在处理网络事件的时候是不能有阻塞操作的，否则很容易导致请求大面积超时。如果实在无法避免使用阻塞操作，那可以通过线程池来异步处理

# Disruptor高性能队列

> **Disruptor 是一款高性能的有界内存队列**，目前应用非常广泛，Log4j2、Spring Messaging、HBase、Storm 都用到了 Disruptor，那 Disruptor 的性能为什么这么高呢

## 高性能的原因

1. 内存分配更加合理，使用 RingBuffer 数据结构，数组元素在初始化时一次性全部创建，提升缓存命中率；对象循环利用，避免频繁 GC。
2. 能够避免伪共享，提升缓存利用率。
3. 采用无锁算法，避免频繁加锁、解锁的性能消耗。
4. 支持批量消费，消费者可以无锁方式消费多个消息。

## 使用

- 在 Disruptor 中，生产者生产的对象（也就是消费者消费的对象）称为 Event，使用 Disruptor 必须自定义 Event，例如示例代码的自定义 Event 是 LongEvent；

- 构建 Disruptor 对象除了要指定队列大小外，还需要传入一个 EventFactory，示例代码中传入的是`LongEvent::new`；

- 消费 Disruptor 中的 Event 需要通过 handleEventsWith() 方法注册一个事件处理器，发布 Event 则需要通过 publishEvent() 方法

  ```
  // 自定义 Event
  class LongEvent {
    private long value;
    public void set(long value) {
      this.value = value;
    }
  }
  // 指定 RingBuffer 大小,
  // 必须是 2 的 N 次方
  int bufferSize = 1024;
   
  // 构建 Disruptor
  Disruptor<LongEvent> disruptor 
    = new Disruptor<>(
      LongEvent::new,
      bufferSize,
      DaemonThreadFactory.INSTANCE);
   
  // 注册事件处理器
  disruptor.handleEventsWith(
    (event, sequence, endOfBatch) ->
      System.out.println("E: "+event));
   
  // 启动 Disruptor
  disruptor.start();
   
  // 获取 RingBuffer
  RingBuffer<LongEvent> ringBuffer 
    = disruptor.getRingBuffer();
  // 生产 Event
  ByteBuffer bb = ByteBuffer.allocate(8);
  for (long l = 0; true; l++){
    bb.putLong(0, l);
    // 生产者生产消息
    ringBuffer.publishEvent(
      (event, sequence, buffer) -> 
        event.set(buffer.getLong(0)), bb);
    Thread.sleep(1000);
  }
  ```

## 伪共享

- 伪共享是因为多个变量保存在同一个缓存行，一个变量改变的时候，导致其它变量一起失效的现象

- 解决方法：每个变量独占一个缓存行、不共享缓存行就可以了，具体技术是**缓存行填充**

- 由于伪共享问题如此重要，所以 Java 也开始重视它了，比如 Java 8 中，提供了避免伪共享的注解：@sun.misc.Contended，通过这个注解就能轻松避免伪共享（需要设置 JVM 参数 -XX:-RestrictContended）

# HiKariCP高性能数据库连接池

- 微观上 HiKariCP 程序编译出的字节码执行效率更高，站在字节码的角度去优化 Java 代码，而宏观上主要是和两个数据结构有关，一个是 FastList，另一个是 ConcurrentBag

## FastList

- HiKariCP 中的 FastList 相对于 ArrayList 的一个优化点就是将 `remove(Object element)` 方法的**查找顺序变成了逆序查找**。除此之外，FastList 还有另一个优化点，是 `get(int index)` 方法没有对 index 参数进行越界检查，HiKariCP 能保证不会越界，所以不用每次都进行越界检查

## ConcurrentBag 

- HiKariCP 并没有使用 Java SDK 中的阻塞队列，而是自己实现了一个叫做 ConcurrentBag 的并发容器。ConcurrentBag 的设计最初源自 C#，它的一个核心设计是使用 ThreadLocal 避免部分并发问题，不过 HiKariCP 中的 ConcurrentBag 并没有完全参考 C# 的实现。ConcurrentBag 中最关键的属性有 4 个，分别是：用于存储所有的数据库连接的共享队列 sharedList、线程本地存储 threadList、等待数据库连接的线程数 waiters 以及分配数据库连接的工具 handoffQueue

- 创建连接：将这个连接加入到共享队列 sharedList 中，如果此时有线程在等待数据库连接，那么就通过 handoffQueue 将这个连接分配给等待的线程
- 获取连接：（需要注意的是，线程本地存储中的连接是可以被其他线程窃取的，所以需要用 CAS 方法防止重复分配。在共享队列中获取空闲连接，也采用了 CAS 方法防止重复分配）
  - 首先查看线程本地存储是否有空闲连接，如果有，则返回一个空闲的连接；
  - 如果线程本地存储中无空闲连接，则从共享队列中获取。
  - 如果共享队列中也没有空闲的连接，则请求线程需要等待。
- 归还连接：首先将数据库连接状态更改为 STATE_NOT_IN_USE，之后查看是否存在等待线程，如果有，则分配给等待线程；如果没有，则将该数据库连接保存到线程本地存储里

- HiKariCP 中的 FastList 和 ConcurrentBag 这两个数据结构使用得非常巧妙，虽然实现起来并不复杂，但是对于性能的提升非常明显，根本原因在于这两个数据结构适用于数据库连接池这个特定的场景。FastList 适用于逆序删除场景；而 ConcurrentBag 通过 ThreadLocal 做一次预分配，避免直接竞争共享资源，非常适合池化资源的分配。

# Actor模型

> Actor 模型本质上是一种计算模型，基本的计算单元称为 Actor，换言之，**在 Actor 模型中，所有的计算都是在 Actor 中执行的**。在面向对象编程里面，一切都是对象；在 Actor 模型里，一切都是 Actor，并且 Actor 之间是完全隔离的，不会共享任何变量

- Java 语言本身并不支持 Actor 模型，所以如果你想在 Java 语言里使用 Actor 模型，就需要借助第三方类库，目前能完备地支持 Actor 模型而且比较成熟的类库就是**Akka**了。
- 在 Actor 模型里，发送消息仅仅是把消息发出去而已，接收消息的 Actor 在接收到消息后，也不一定会立即处理，也就是说**Actor 中的消息机制完全是异步的**。而**调用对象方法**，实际上是**同步**的，对象方法 return 之前，调用方会一直等待。
- 除此之外，**调用对象方法**，需要持有对象的引用，**所有的对象必须在同一个进程中**。而在 Actor 中发送消息，类似于现实中的写信，只需要知道对方的地址就可以，**发送消息和接收消息的 Actor 可以不在一个进程中，也可以不在同一台机器上**。因此，Actor 模型不但适用于并发计算，还适用于分布式计算

- 在 Java 领域，除了可以使用 Akka 来支持 Actor 模型外，还可以使用 Vert.x，不过相对来说 Vert.x 更像是 Actor 模型的隐式实现，对应关系不像 Akka 那样明显，不过本质上也是一种 Actor 模型
- Actor 可以创建新的 Actor，这些 Actor 最终会呈现出一个树状结构，非常像现实世界里的组织结构，所以利用 Actor 模型来对程序进行建模，和现实世界的匹配度非常高。Actor 模型和现实世界一样都是异步模型，理论上不保证消息百分百送达，也不保证消息送达的顺序和发送的顺序是一致的，甚至无法保证消息会被百分百处理。虽然实现 Actor 模型的厂商都在试图解决这些问题，但遗憾的是解决得并不完美，所以使用 Actor 模型也是有成本的

# 软件事务内存

> 很多编程语言都有从数据库的事务管理中获得灵感，并且总结出了一个新的并发解决方案：**软件事务内存（Software Transactional Memory，简称 STM）**。传统的数据库事务，支持 4 个特性：原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持久性（Durability），也就是大家常说的 ACID，STM 由于不涉及到持久化，所以只支持 ACI

- Java 语言并不支持 STM，不过可以借助第三方的类库来支持，Multiverse就是个不错的选择
- 有不少 STM 的实现方案都是基于 MVCC 的，例如知名的 Clojure STM

- STM 借鉴的是数据库的经验，数据库虽然复杂，但仅仅存储数据，而编程语言除了有共享变量之外，还会执行各种 I/O 操作，很显然 I/O 操作是很难支持回滚的。所以，STM 也不是万能的。目前支持 STM 的编程语言主要是函数式语言，函数式语言里的数据天生具备不可变性，利用这种不可变性实现 STM 相对来说更简单

# 协程

> 我们可以把**协程**简单地理解**为一种轻量级的线程**。从操作系统的角度来看，线程是在内核态中调度的，而协程是在用户态调度的，所以相对于线程来说，协程切换的成本更低。协程虽然也有自己的栈，但是相比线程栈要小得多，典型的线程栈大小差不多有 1M，而协程栈的大小往往只有几 K 或者几十 K。所以，无论是从时间维度还是空间维度来看，协程都比线程轻量得多。
>
> Golang、Python、Lua、Kotlin 等都支持协程，Java OpenSDK 中 Loom 项目的目标就是支持协程

# CSP模型

> Golang解决协作问题，Golang 提供了两种不同的方案：一种方案支持协程之间以共享内存的方式通信，Golang 提供了管程和原子类来对协程进行同步控制，这个方案与 Java 语言类似；另一种方案支持协程之间以消息传递（Message-Passing）的方式通信，本质上是要避免共享，Golang 的这个方案是基于**CSP**（Communicating Sequential Processes）模型实现的，Golang推荐后者

## CSP 模型与 Actor 模型的区别

- 第一个最明显的区别就是：**Actor 模型中没有 channel**。虽然 Actor 模型中的 mailbox 和 channel 非常像，看上去都像个 FIFO 队列，但是区别还是很大的。Actor 模型中的 mailbox 对于程序员来说是“透明”的，mailbox 明确归属于一个特定的 Actor，是 Actor 模型中的内部机制；而且 Actor 之间是可以直接通信的，不需要通信中介。但 CSP 模型中的 channel 就不一样了，它对于程序员来说是“可见”的，是通信的中介，传递的消息都是直接发送到 channel 中的
- 第二个区别是：Actor 模型中发送消息是**非阻塞**的，而 CSP 模型中是**阻塞**的。Golang 实现的 CSP 模型，channel 是一个阻塞队列，当阻塞队列已满的时候，向 channel 中发送数据，会导致发送消息的协程阻塞
- 第三个区别则是关于消息送达的。Actor 模型理论上不保证消息百分百送达，而在 Golang 实现的**CSP 模型中，是能保证消息百分百送达的**。不过这种百分百送达也是有代价的，那就是有可能会导致**死锁**

- Java 领域可以借助第三方的类库JCSP来支持 CSP 模型，相比 Golang 的实现，JCSP 更接近理论模型

------

# 进程与线程

> 进程是资源分配的最小单位，线程是CPU调度的最小单位

- 所有与进程相关的资源，都被记录在PCB中
- 进程是抢占处理机的调度单位；线程属于某个进程，共享其资源
- 线程只由堆栈寄存器、程序计数器、和TCB组成

## JVM中的进程与线程

- Java对OS提供的功能进行封装，包括进程与线程

- 每个进程对应一个JVM实例，多个线程共享JVM里面的堆
- Java采用单线程编程模式，程序会主动创建主线程

## run与start

- run只是Thread的一个普通方法调用，会沿用主线程执行程序
- start会创建并启动一个新的子线程来执行程序
- `Thread#srart()`->`JVM_StartThread`->`thread_entry`->`Thread#run()`

## Thread与Runnable

> 可以继承Thread来进行多线程编程，也可以实现Runnable接口，推荐后者

- Thread是一个类，Runnable是一个接口，Thread也实现了该接口
- Thread是实现了Runnable接口的类，使得run支持多线程

## sleep与wait

- sleep是Thread的方法（线程暂停），wait是Object的方法
- sleep方法可以在任何地方使用，wait只能在synchronized方法或者块中使用
- sleep只让出CPU，不释放锁；wait不仅让出CPU，还会释放已经占有的同步资源

## notify与notifyAll

- 锁池 `EntryList`，当锁无法获取某对象的锁，其会进入锁池中
- 等待池 `WaitSet`，如果一个线程调用了某对象的wait方法，该线程就会进入等待池中，不去竞争该对象的锁
- notifyAll会让所有处于等待池的线程全部进入锁池去竞争锁
- notify会**随机**选取一个在等待池中的线程进入锁池去竞争锁

## yield

> 屈服，放弃

- 当调用`Thread.yield()`时，会给线程调度器一个当前线程愿意让出CPU的暗示，但是线程调度器有可能会忽略这个暗示
- 调用yield，不会释放锁资源（不会影响锁的行为）

## 中断线程

1. `stop()`、`suspend()`、`resume()`已被废弃
2. 调用interrupt()，通知线程应该中断了
   1. 如果线程处于阻塞状态，那么该线程会立即退出阻塞状态并抛出`InterruptedException`异常
   2. 如果线程处于正常活动状态，那么该线程的中断标记位被设置为true，被设置中断标记的线程将继续正常运行不受影响
   3. 需要被调用的线程配合才可以实现中断：轮询中断标记位进行相应的操作

## 如何给run()方法传参

1. 构造函数
2. 成员变量
3. 回调函数

## 处理线程返回值（处理结果）

1. 主线程等待法（自旋轮询）

2. 使用Thread类的join方法阻塞当前线程以等待子线程处理完毕

3. 通过Callable接口实现：通过`FutureTask`或者线程池获取

   ```
   实现Callable接口下的call()方法
   
   使用线程池
   ```

## 线程的状态

1. New 创建后尚未启动的线程的状态
2. Runnable 包含Running与Ready状态
3. Waiting 无限等待，需要被显示唤醒；
   1. 没有timeout参数的`Object.wait()`
   2. 没有timeout参数的`Thread.join()`
   3. `LockSupport.park()`
4. Timed Waiting 限期等待
   1. `Thread.sleep()`
   2. 设置了timeout参数的`Object.wait()`
   3. 设置了timeout参数的`Thread.join()`
   4. `LockSupport.parkNanos()`方法
   5. `LockSupport.parkUntil()`方法
5. Blocked 等待获取排他锁
6. Terminated 终止，已终止，线程已经结束执行

# 互斥锁

- 互斥性：同一时间只允许一个线程持有某个对象锁，通过这种特性来实现多线程的协调机制，也称为操作的原子性
- 可见性：必须确保锁被释放之前，对共享变量的修改，对于其后获取到该锁的另一个线程是可见的，即获取锁时应该获取到最新的共享变量的值

## synchronized

- synchronized锁的不是代码而是对象
- 同一个类的不同对象锁互不干扰；同一个类的不同对象使用类锁将会是同步的

### 获取对象锁

1. 同步代码块，锁是小括号中代表的实例对象

   ```
   synchronized(this)
   
   synchronized(类对象实例)
   ```

2. 同步非静态方法（synchronized method），锁是当前对象的实例对象

### 获取类锁

> 实际上通过对象锁实现，对应类的Class对象

1. 同步代码块，锁是小括号中的类对象（Class对象）

   ```
   synchronized(类.class)
   ```

2. 同步静态方法（synchronized static method），锁是当前对象的类对象（Class对象）

### 底层实现

#### 对象头

- 对象的内存布局
  - 对象头
    - Mark Word 默认储存对象的`hashCode`，分代年龄，锁类型，锁标志位等信息（非固定的数据结构）
    - Class Metadata Address 类型指针指向对象的类元数据，JVM通过这个指针确定该对象是哪个类的实例
  - 实例数据
  - 对齐填充

#### Monitor

> 每个Java对象天生自带了一把看不见的锁，即管程，也称内部锁

- 部分数据结构
  - count
  - owner
  - `WaitSet`
  - `EntryList`

#### 重入

- 当一个线程再次请求自己持有对象锁的临界资源时，称为重入

- 在Java中，synchronized基于原子性的内部锁机制，是可重入的

#### 字节码解读

- 同步代码块 `monitorenter`~`monitorexit`，编译器会自动产生一个异常处理器，可以处理所有的异常，用来执行`monitorexit`指令
- 同步方法 同步是隐式的，无需通过字节码指令来控制，用`ACC_SYNCHRONIZED`用来标记是否是同步方法。调用方法时，会检测该标志，如果该标志位被标志，该该线程持有Monitor，再执行方法。无论方法是否正常退出，Monitor都会被释放。

#### 历史变迁

- 早期版本中，synchronized属于重量级锁，依赖于Mutex Lock实现；线程切换之间需要从用户态切换到内核态，开销较大
- JDK6+，做了锁优化：锁自旋，锁消除，锁粗化，轻量级锁，偏向锁

#### 状态

> 锁其实会降级，当JVM进入安全点时，会检查是否由闲置的Monitor，尝试进行降级

1. 无锁
2. 偏向锁：减少同一个线程获取锁的代价
   1. 原因：大多数情况下，锁不存在多线程竞争，总是由同一个线程多次获得
   2. 思想：如果一个线程获取了锁，该锁进入偏向锁模式，当该线程再次请求锁时，无需做同步操作。获取锁的过程只需要检查Mark Word的锁标记是否为偏向锁且当前线程ID等于Mark Word中的Thread Id即可
   3. 适用于单线程无竞争场景
3. 轻量级锁
   1. 偏向锁运行在一个线程进入同步块的情况下，其它线程加入锁竞争的时候，偏向锁会升级为轻量级锁
   2. 适用于线程交替执行同步块的场景
4. 重量级锁

### 自旋锁

- 许多情况下，共享数据的锁定状态持续时间较短，不值得切换线程
- 通过让线程执行忙循环等待锁的释放，不让出CPU
- JDK4出现但默认关闭，JDK6+默认开启
- 缺点：如果锁被其它线程占用时间特别长，会带来许多性能上的开销
- `PreBlockSpin`配置参数

### 自适应自旋锁

- 自旋的次数不再固定，由前一次在同一个锁上的自旋时间以及锁的拥有者状态来决定

### 锁消除

- JIT编译时，对运行上下文进行扫描，去除不可能存在竞争的锁

### 锁粗化

- 通过扩大加锁范围，避免频繁加锁与解锁。如在循环体内的同步

  ```
  StringBuffer sb = new StringBuffer();
  while(...){
  	ab.append(...);
  }
  ```

### 锁的内存语义

1. 当线程释放锁时，JMM会把该线程对应的本地内存中的共享变量刷新到主存中
2. 当线程获取锁时，JMM会把该线程对应的本地内存置为无效，从而使得被监视器保护的临界区代码必须从内存中读取共享变量

## ReentrantLock

> 可重入锁

- 基于AQS实现
- 能够实现比synchronized更细粒度的控制，如实现公平锁（将锁赋予等待时间最久的线程，避免饥饿）
- 性能未必比synchronized高
- 将锁对象化
  - 判断是否有线程或者某个特定线程在排队等待获取锁
  - 带超时的获取锁的尝试
  - 感知是否成功获取锁

# JMM

> Java内存模型，本身是一种抽象的概念并不真实存在，它描述的是一组规则规范，通过这组规则定义了程序中各个变量（实例字段、静态字段、数组对象）的访问方式

- 每个线程在创建时，会从主存拷贝一份共享内存变量到自己的工作空间（本地内存），本地变量对其它线程不可见
- **JMM与JVM内存区域划分是不同的概念层次**：JMM描述的是一组规则，围绕原子性‘有序性、可见性展开；相似点：都有共享区域和私有区域

### happens-before原则（8大）

1. 程序次序原则：**一个线程内**，按照代码顺序，书写在前面的操作先于书写在后面的操作（结果是有序的）
2. 锁定规则：一个unlock操作先行发生于后面对于同一个锁的lock操作（解锁必须要对加锁可见，不然可能造成多个线程加锁）
3. volatile变量原则：对一个volatile变量的写操作先行发生于后面对这个变量的读操作
4. 传递性
5. 线程启动规则：`Threa.satrt()`方法先行发生于此线程的每一个动作
6. 线程中断原则：对于线程interrupt方法的调用先行发生于被中断线程的代码检测到中断事件的发生
7. 线程终止规则：线程中所有的操作均先行发生于线程的终止检测
8. ~~对象终结规则：一个对象的初始化完成先行发生于其finalize方法的开始~~

### 重排序

#### 条件

> 无法通过happens-before原则推导出来的，才可以进行指令的重排序

- 在单线程环境下不能改变程序运行的结果
- 存在数据依赖关系的指令不允许重排序

## volatile

> JVM提供的轻量级同步机制

- 保证volatile修饰的共享变量对所有线程总是可见的（当一个线程修改了被volatile修饰的变量的值的时候，其它线程立即感知到变动）

- 禁止指令的重排序优化

- 被volatile修饰的变量，对其它线程总是立即可见的。对volatile变量的所有修改，总是立即反映到其它线程中。**但是在多线程环境下，对于volatile变量做运算操作，并不保证其安全性**

  ```
  // 线程不安全
  public class Demo{
  	public static volatile int val = 0;
  	public static void increase(){
  		val++;
  	}
  }
  
  // 必须使用synchronized进行访问控制，volatile可以去掉（语义重复）
  public class Demo{
  	public static int val = 0;
  	public synchronized static void increase(){
  		val++;
  	}
  }
  
  // 提供volatile解决内存可见性，（对boolean类型的修改是原子性的）
  public class Demo{
  	public static volatile boolean flag = true;
  	public static void close(){
  		flag = false;
  	}
  	public static void work(){
  		while(flag){
  			// do something...
  		}
  	}
  }
  ```

### 原理

- 线程读取被volatile修饰的变量时，JMM会把对应的工作内存设置为无效，需要到主存获取最新值
- 写一个volatile修饰的变量时，JMM会将其对应的工作内存的共享变量刷新到主存

###如何禁止重排序

- 内存屏障（Memory Barrier）
  - 保证特定操作的执行顺序（通过插入内存屏障指令禁止在内存屏障前后的指令执行重排序优化）
  - 保证某些变量的内存可见性（强制刷新CPU的缓存数据，因此任何CPU上的线程都能读取这些数据的最新版本）

## volatile与synchronized

- volatile仅能作用于变量；synchronized可以使用在变量、方法、类级别
- volatile仅可以保证内存可见性，不能保证原子性；synchronized可以保证原子性与可见性
- volatile不会造成线程的阻塞，synchronized有可能阻塞
- volatile变量不会被编译器优化

# CAS

> Compare and Swap，一种高效实现线程安全性的方法

- 支持原子更新操作，适用于计数器，序列发生器等场景
- 属于乐观锁，号称`lock-free`
- CAS失败时由开发者决定是继续尝试还是执行别的操作

## 思想

- 包含三个操作数
  - 内存位置V
  - 预期原值A
  - 新值B
- 执行CAS操作时，将内存位置的值与预期原值A比较，如果一致，则将该值修改为新值B
- 可以使用Atomic为volatile提供原子性
- CAS多数情况下对于程序员的透明的
- 缺点：若循环时间长，开销很大；只能保证一个共享变量的原子性操作；ABA问题（解决：`AtomicStampedReference`）

# 线程池

> 利用Executors创建不同的线程池满足不同场景的需求

## 使用线程池的好处

1. 降低资源消耗
2. 提高线程的可管理性

## FixedThreadPool

> 指定工作线程数数量的线程池

## CachedThreadPool

> 处理大量短时间工作任务的线程池

- 试图**缓存**线程并重用，当无缓存线程可用时，就会创建新的工作线程
- 如果线程空闲的时间超过阈值，则会被终止并移出缓存
- 系统长时间闲置的时候，不会消耗太多的资源

## SingleThreadExecutor

> 创建唯一的工作者线程来执行任务，如果线程异常结束，会有另一个线程取代它

## SingleThreadScheduledExecutor

> 单线程定时周期性调度

## ScheduledThreadPool

> 多线程定时周期性调度

## WorkStealingPool

> 内部会构建ForkJoinPool，利用working-stealing算法，并行地处理任务，不保证处理顺序（JDK8+）

## JUC包的三个Executor接口

- Executor：运行新任务的简单接口，将任务提交和任务执行细节解耦
- ExecutorService：具备管理执行器和任务生命周期的方法，提交任务机制更完善
- ScheduledExecutorService：支持Future和定期执行任务

## ThreadPoolExecutor

### 构造函数

- corePoolSize 核心线程数
- maximumPoolSize 线程不够用时能创建的最大线程数
- workQueue 任务等待队列
- keepAliveTime 空闲线程的最大存活时间
- threadFactory 创建新的线程 `Executors.defaultThreadFactory()`
- handler 线程池的饱和策略
  - AbortPolicy 直接抛出异常（默认设置）
  - CallerRunsPolicy 用调用者所在的线程来执行任务
  - DiscardOldestPolicy 丢弃任务队列中最靠前的任务，并执行当前任务
  - DiscardPolicy 直接丢弃任务
  - 实现RejectedExecutionHandler接口的自定义handler

## 线程池的状态

- RUNNING 能够接受新提交的任务，并且也能处理阻塞队列中的任务
- SHUTDOWN 不再接受新提交的任务，但是可以处理存量任务
- STOP 不接受新任务，也不处理旧任务
- TIDYING 所有的任务都已经终止
- TERMINATED terminated方法执行完成之后进入该状态

# Fork/Join框架

> 把大任务分割成若干小任务并执行，最终汇总每个小任务结果后得到大任务结果的框架，原理类似于Map Reduce

- 工作窃取算法：某个线程从其它队列里窃取任务来执行，已完成工作任务的线程会从其它工作队列获取其它工作任务

