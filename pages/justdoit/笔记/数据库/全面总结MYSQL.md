# MYSQL架构

![MySQL的逻辑架构图](./MySQL的逻辑架构图.png)

## 总体

- MySQL可以分为Server层和存储引擎层两部分。Server层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持InnoDB、MyISAM、Memory等多个存储引擎。现在最常用的存储引擎是InnoDB，它从MySQL 5.5.5版本开始成为了默认存储引擎。

## 连接器

- 连接命令 `mysql -h$ip -P$port -u$user -p`

- 连接->验证账号密码->查询权限表确认权限（这就意味着，一个用户成功建立连接后，即使用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置）

- `show processlist`查询当前的连接

- `wait_timeout`参数控制自动断开时长，默认8小时

- 连接按连接保持时长有长连接和短链接之分

  > 建立连接的过程通常是比较复杂的，所以我建议你在使用中要尽量减少建立连接的动作，也就是尽量使用长连接。
  >
  > 但是全部使用长连接后，你可能会发现，有些时候MySQL占用内存涨得特别快，这是因为MySQL在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是MySQL异常重启了。
  >
  > 怎么解决这个问题呢？你可以考虑以下两种方案。
  >
  > 1. 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。
  > 2. 如果你用的是MySQL 5.7或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。

## 查询缓存

- 在内存中以key-value形式保存，key为sql语句，value为结果

- 大多数情况下不建议使用查询缓存：查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空

  > MySQL也提供了这种“按需使用”的方式：将参数query_cache_type设置成DEMAND，这样对于默认的SQL语句都不使用查询缓存。而对于你确定要使用查询缓存的语句，可以用SQL_CACHE显式指定，像下面这个语句一样：
  >
  > ```
  >mysql> select SQL_CACHE * from T where ID=10；
  > ```
  
- **MySQL 8.0版本直接将查询缓存的整块功能删掉了，也就是说8.0开始彻底没有这个功能了**

## 分析器

- 词法分析（识别语句类型，字段名称，表名字等等）->语法分析（是否符合MYSQL语法）
- 一般语法错误会提示第一个出现错误的位置，所以要关注的是紧接“use near”的内容

## 优化器

- 优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序

## 执行器

- 权限查询->打开表，根据其引擎定义使用其接口
- 慢查询日志中`rows_examined`的字段表示这个语句执行过程中扫描了多少行，注意引擎扫描行数跟rows_examined并不是完全相同的。

# 查询语句执行原理

1. 连接，检查权限
2. 查询缓存（将废弃）
3. 分析SQL语句
4. 优化SQL语句，确定执行计划
5. 确认权限，执行器执行

# 更新语句执行原理

> 增删改

1. 连接
2. 将相关表的缓存清空
3. 分析SQL语句
4. 优化SQL语句，确定执行计划
5. 执行器执行，记录日志

## redo log

> InnoDB引擎独有，相当于备忘录

- WAL技术，Write Ahead Logging，先写日志，再写磁盘。即当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log里面，并更新内存，这个时候更新就算完成了。同时，InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做
- redo log大小固定，循环写
- 双指针 check point、write point
  - checkpoint，当前要擦除的位置
  - write point，当前记录位置
  - write point 和 checkpoint 之间的是 “ 备忘录 ” 上还空着的部分，可以用来记录新的操作。如果 write point追上 checkpoint ，表示 “ 备忘录” 满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把check point 推进一下
- crash-safe：有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为**crash-safe**

### flush时机

1. **redolog写满时**
2. **内存空间不足**
3. 系统空闲时
4. MySQL服务正常停止时

### buffer pool

- **InnoDB用缓冲池（buffer pool）管理内存，缓冲池中的内存页有三种状态：**
  - 第一种是，还没有使用的；
  - 第二种是，使用了并且是干净页；
  - 第三种是，使用了并且是脏页
- 当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。InnoDB的策略是尽量使用内存，因此对于一个长时间运行的库来说，未被使用的页面很少
- InnoDB的刷盘速度就是要参考这两个因素：一个是脏页比例，一个是redo log写盘速度

### 刷盘

- 正常运行中的实例，数据写入后的最终落盘，是从redo log更新过来的还是从buffer pool更新过来的呢：实际上，redo log并没有记录数据页的完整数据，所以它并没有能力自己去更新磁盘数据页，也就不存在“数据最终落盘，是由redo log更新过去”的情况。
  1. 如果是正常运行的实例的话，数据页被修改以后，跟磁盘的数据页不一致，称为脏页。最终数据落盘，就是把内存中的数据页写盘。这个过程，甚至与redo log毫无关系。
  2. 在崩溃恢复场景中，InnoDB如果判断到一个数据页可能在崩溃恢复的时候丢失了更新，就会将它读到内存，然后让redo log更新内存内容。更新完成后，内存页变成脏页，就回到了第一种情况的状态。

## bin log

> Server层日志，归档日志，没有crash-safe功能

- binlog有三种记录模式，statement格式的话是记sql语句； row格式会记录行的内容，记两条，更新前和更新后都有；mixed格式MySQL默认仍然采用statement格式进行记录，但是一旦它判断可能会有数据不一致的情况发生，则会采用row格式来记录

## redo log与bin log区别

1. redo log是InnoDB引擎特有的；binlog是MySQL的Server层实现的，所有引擎都可以使用
2. redo log是物理日志，记录的是“在某个数据页上做了什么修改”；binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”
3. redo log是循环写的，空间固定会用完；binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志

## 操作顺序

1. 搜索，如果数据不在内存，将其读入内存
2. 调用引擎接口更新数据，保存到内存中，同时提交记录到redolog，此时redolog状态为prepare（第一阶段提交）
3. 执行器生成该操作的binlog，将binlog写入磁盘
4. 执行器调用引擎接口提交事务，此时redolog状态为commit，更新完毕（第二阶段提交）

## 两阶段提交

> redolog的两阶段提交目的是为了让两份日志逻辑保持一致

- 由于redo log和binlog是两个独立的逻辑，如果不用两阶段提交，要么就是先写完redo log再写binlog，或者采用反过来的顺序，都会造成数据的不一致
  - 先写redo log，bin log失效，使用bin log备份恢复时缺失事务
  - 先写bin log，redo log失效，使用bin log备份恢复时多余事务

## 落盘策略

> 通常我们说MySQL的“双1”配置，指的就是sync_binlog和innodb_flush_log_at_trx_commit都设置成 1。也就是说，一个事务完整提交前，需要等待两次刷盘，一次是redo log（prepare 阶段），一次是binlog

### binlog

- `sync_binlog`
  - sync_binlog=0的时候，表示每次提交事务都只write，不fsync；
  - sync_binlog=1的时候，表示每次提交事务都会执行fsync；
  - sync_binlog=N(N>1)的时候，表示每次提交事务都write，但累积N个事务后才fsync

### redolog

- `innodb_flush_log_at_trx_commit `
  - 设置为0的时候，表示每次事务提交时都只是把redo log留在redo log buffer中;
  - 设置为1的时候，表示每次事务提交时都将redo log直接持久化到磁盘；（如果把innodb_flush_log_at_trx_commit设置成1，那么redo log在prepare阶段就要持久化一次，因为有一个崩溃恢复逻辑是要依赖于prepare 的redo log，再加上binlog来恢复的）
  - 设置为2的时候，表示每次事务提交时都只是把redo log写到page cache
- 其它时机
  - InnoDB有一个后台线程，每隔1秒，就会把redo log buffer中的日志，调用write写到文件系统的page cache，然后调用fsync持久化到磁盘
  - **redo log buffer占用的空间即将达到 innodb_log_buffer_size一半的时候，后台线程会主动写盘。**注意，由于这个事务并没有提交，所以这个写盘动作只是write，而没有调用fsync，也就是只留在了文件系统的page cache
  - **并行的事务提交的时候，顺带将这个事务的redo log buffer持久化到磁盘。**假设一个事务A执行到一半，已经写了一些redo log到buffer中，这时候有另外一个线程的事务B提交，如果innodb_flush_log_at_trx_commit设置的是1，那么按照这个参数的逻辑，事务B要把redo log buffer里的日志全部持久化到磁盘。这时候，就会带上事务A在redo log buffer里的日志一起持久化到磁盘

### 组提交

> 从MySQL看到的TPS是每秒两万的话，每秒就会写四万次磁盘。但是用工具测试出来，磁盘能力也就两万左右，怎么能实现两万的TPS？

- 日志逻辑序列号（log sequence number，LSN）：LSN是单调递增的，用来对应redo log的一个个写入点。每次写入长度为length的redo log， LSN的值就会加上length
- 在一组并发事务中，最先到达的事务被选为leader，组长落盘的时候，将最大的LSN（M）落盘，等到组长返回的时候，所有LSN小于等于M的redo log，都已经被持久化到磁盘，其它组员事务可以直接返回。所以，一次组提交里面，组员越多，节约磁盘IOPS的效果越好。但如果只有单线程压测，那就只能老老实实地一个事务对应一次持久化操作了

# 事务隔离

- 事务的特性： ACID （ Atomicity 、 Consistency 、 Isolation 、 Durability ，即原子性、一致性、隔离性、持久性）

- 并发事务的问题：脏读（ dirty read ）、不可重复读（ non-repeatable read ）、幻读（ phantom read ）

-  SQL 标准的事务隔离级别：读未提交（ read uncommitted ）、读提交（ read committed ）、可重复读（ repeatable read ）和串行化（ serializable  ）

- > 在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。在“读提交”隔离级别下，这个视图是在每个SQL语句开始执行的时候创建的。这里需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念；而“串行化”隔离级别下直接用加锁的方式来避免并行访问。

- 参数` transaction-isolation` 
- 在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。(回滚日志)
- 同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（ MVCC ）
- 回滚日志在不需要的时候才删除。也就是说，系统会判断，当没有事务再需要用到这些回滚日志时，回滚日志会被删除，就是当系统里没有比这个回滚日志更早的 read-view 的时候。

- 不建议使用长事务，有些客户端连接框架会默认连接成功后先执行一个set autocommit=0的命令。这就导致接下来的查询都在事务中，如果是长连接，就导致了意外的长事务。

- 事务的启动方式

  > 1.  显式启动事务语句， begin  或 start transaction 。配套的提交语句是 commit ，回滚语句是
  > rollback 。
  > 2. set autocommit=0 ，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个
  > select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行
  > commit  或 rollback  语句，或者断开连接。

- 设置`autocommit=1`在 autocommit 为 1 的情况下，用 begin 显式启动的事务，如果执行 commit 则提交事务。如果执行**commit work and chain** ，则是提交事务并自动启动下一个事务，这样也省去了再次执行 begin 语句的开销。同时带来的好处是从程序开发的角度明确地知道每个语句是否处于事务中。

- 在 information_schema 库的 innodb_trx 这个表中查询长事务

  ```
  select * from information_schema.innodb_trx where TIME_TO_SEC(timediff(now(),trx_started))>60
  ```

# 索引

- 常见的索引模型
  - 哈希表，适合等值查询如Nosql
  - 有序数组，适合静态储存引擎，更新时低效
  - N叉树，MYSQL中N一般是1200，树根在内存之中。以 InnoDB 的一个整数字段索引为例，这个 N 差不多是 1200 。这棵树高是 4 的时候，就可以存1200 的 3 次方个值，这已经 17 亿了。考虑到树根的数据块总是在内存中的，一个 10 亿行的表上一个整数字段的索引，查找一个值最多只需要访问 3 次磁盘。其实，树的第二层也有很大概率在内存中，那么访问磁盘的平均次数就更少了

## InnoDB索引模型

> 在 MySQL 中，索引是在存储引擎层实现的，所以并没有统一的索引标准，即不同存储引擎的索
> 引的工作方式并不一样。而即使多个存储引擎支持同一种类型的索引，其底层的实现也可能不
> 同。

- 在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。
  又因为前面我们提到的， InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的。

- 索引类型分为主键索引和非主键索引主键索引的叶子节点存的是整行数据。在 InnoDB 里，主键索引也被称为聚簇索引（ clusteredindex ）。非主键索引的叶子节点内容是主键的值。在 InnoDB 里，非主键索引也被称为二级索引（ secondary index ）。
  
- 基于非主键索引的查询需要多扫描一棵索引树（普通索引查询方式，则需要先搜索 k 索引树，得到主键
  的值为 ，再到 主键索引树搜索一次，这个过程称为**回表**）。因此，我们在应用中应该尽量使用主键查询

## 索引维护

- 页分裂，页合并
- 每个表都必须有自增主键的原因：自增主键的插入数据模式，正符合了我们前面提到的递增插入的场景。每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂。而有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高。即使有的业务字段是唯一的，还是需要使用自增主键。此外，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小
- 如果只有一个索引而且该索引必须是唯一索引，即KeyValue场景，可以将该key设置为主键，可以避免每次查询需要搜索两棵树，由于没有其他索引，所以也就不用考虑其他索引的叶子节点大小的问题。

## 覆盖索引

- 由于查询结果所需要的数据只在主键索引上有，所以不得不回表。
- `select id`而不是`select *`，索引k已经“覆盖了”我们的查询需求，我们称为覆盖索引
- 如：需要频繁地根据用户ID获取其姓名，可以对ID以及姓名建立**联合索引**，那么在查询的时候，不需要回表（索引覆盖）

## 最左前缀原则

- 不必要为低频的查询创建索引
- 不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符
- 在建立联合索引的时候，如何安排索引内的字段顺序？考虑索引的复用能力
- 在建立联合索引的时候，如何安排索引内的字段顺序：第一原则是，如果通过调整顺序，可以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的，第二原则虑的原则就是空间

## 联合索引的索引下推

- （MYSQL5.6以后）在索引遍历过程中，对**索引**中包含的字段先做判断，直接**过滤掉不满足条件的记录**，减少回表次数

## 重建索引

- 索引可能因为删除，或者页分裂等原因，导致数据页有空洞，重建索引的过程会创建一个新的索引，把数据按顺序插入，这样页面的利用率最高，也就是索引更紧凑、更省空间

## 普通索引与唯一索引

- 对于长度很长的字段，不建议将其作为主键，因为这样会导致其它非主键索引也变得很庞大

### 查询过程

InnoDB的数据是按数据页为单位来读写的。也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在InnoDB中，每个数据页的大小默认是16KB。如果是唯一索引，找到数据即可返回；如果是普通索引，需要扫描直到遇到第一条不匹配的数据（索引的叶子结点有序）。如果某条记录刚好是这个数据页的最后一个记录，那么要取下一个记录，必须读取下一个数据页，这个操作会稍微复杂一些。但是，我们之前计算过，对于整型字段，一个数据页可以放近千个key，因此出现这种情况的概率会很低。所以，我们计算平均性能差异时，仍可以认为这个操作成本对于现在的CPU来说可以忽略不计

### 更新过程

- 当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InooDB会将这些更新操作缓存在change buffer中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性。

  需要说明的是，虽然名字叫作change buffer，实际上它是可以持久化的数据。也就是说，change buffer在内存中有拷贝，也会被写入到磁盘上。

  将change buffer中的操作应用到原数据页，得到最新结果的过程称为merge。除了**访问这个数据页**会触发merge外，系统有后台线程会**定期**merge。在数据库**正常关闭**（shutdown）的过程中，也会执行merge操作。

  显然，如果能够将更新操作先记录在change buffer，减少读磁盘，语句的执行速度会得到明显的提升。而且，数据读入内存是需要占用buffer pool的，所以这种方式还能够避免占用内存，提高内存利用率。

- 对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。比如，要插入(4,400)这个记录，就要先判断现在表中是否已经存在k=4的记录，而这必须要将数据页读入内存才能判断。如果都已经读入到内存了，那直接更新内存会更快，就没必要使用change buffer了。因此，唯一索引的更新就不能使用change buffer，实际上也只有普通索引可以使用。
- change buffer用的是buffer pool里的内存，因此不能无限增大。change buffer的大小，可以通过参数innodb_change_buffer_max_size来动态设置。这个参数设置为50的时候，表示change buffer的大小最多只能占用buffer pool的50%
- 当要写的记录在内存中时，唯一索引以及普通索引性能差异不大。而当这个记录要更新的目标页不在内存中。这时，InnoDB的处理流程如下：
  - 对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束；
  - 对于普通索引来说，则是将更新记录在change buffer，语句执行就结束了。
- 如果所有的更新后面，都马上伴随着对这个记录的查询，那么你应该关闭change buffer。而在其他情况下，change buffer都能提升更新性能

## 优化器选错索引

> 优化器在选择索引的时候，需要考虑扫描数据量、回表次数、是否需要排序等方面

- 对于由于索引统计信息不准确导致的问题，你可以用analyze table来解决
- 而对于其他优化器误判的情况，你可以在应用端用force index来强行指定索引，也可以通过修改语句来引导优化器，还可以通过增加或者删除索引来绕过这个问题

## 对字符串建立索引

- 可以只对字段的指定部分建立索引，如`alter table tb add index idx_email(email(6));`，但是这样有可能会增加回表的次数，而且有可能导致索引覆盖失效（前缀索引）

- 一般由区分度决定索引的截取长度，可以使用如下语句辅助决定（基数）

  ```
  select
    count(distinct email) as L,
    count(distinct left(email,4)）as L4,
    count(distinct left(email,5)）as L5,
    count(distinct left(email,6)）as L6,
    count(distinct left(email,7)）as L7 from SUser;
  ```

- 对于前面部分区分度不够的字段，可以倒序储存或者对字段进行哈希字段（如使用crc32函数），这两个方法均不能进行范围查询

# 锁

## 全局锁

- MySQL 提供了一个加**全局读锁**的方法，命令是Flush tables with read lock (FTWRL) 
  
- 全局锁的典型使用场景是做**全库逻辑备份**

- 官方自带的逻辑备份工具是 mysqldump 。当 mysqldump 使用参数 –single-transaction 的时候，导数据之前就会启动一个事务，来确保拿到**一致性视图**。而由于 MVCC 的支持，这个过程中数据是可以正常更新的。 single-transaction 方法只适用于所有的表使用事务引擎的库。如果有的表使用了不支持事务的引擎，那么备份就只能通过 FTWRL 方法。这往往是 DBA 要求业务开发人员使用InnoDB 替代 MyISAM 的原因之一

- readonly方式也可以让全库进入只读状态，但还是建议用FTWRL方式，主要有两个原因：

  - 一是，在有些系统中，readonly的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改global变量的方式影响面更大，我不建议你使用。
  - 二是，在异常处理机制上有差异。如果执行FTWRL命令之后由于客户端发生异常断开，那么MySQL会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为readonly之后，如果客户端发生异常，则数据库就会一直保持readonly状态，这样会导致整个库长时间处于不可写状态，风险较高。
  
- 读内存中存在的数据页的时候，直接从内存返回。WAL之后如果读数据，不一定要读盘，不一定要从redo log里面把数据更新以后才可以返回。虽然磁盘上还是之前的数据，但是这里直接从内存返回结果，结果是正确的

- 要读内存中不存在的数据页的时候，需要把该页从磁盘读入内存中，然后应用change buffer里面的操作日志，生成一个正确的版本并返回结果，然后写磁盘即merge。

- **redo log 主要节省的是随机写磁盘的IO消耗（转成顺序写），而change buffer主要节省的则是随机读磁盘的IO消耗**

## 表级锁

- MySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁（ meta data lock ， MDL)
- 表锁的语法是 **lock tables … read/write**
- **MDL 不需要显式使用，在访问一个表的时候会被自动加上。**MDL 的作用是，保证读写的正确性，在读取数据期间不允许修改表结构
- 在MySQL 5.5版本中引入了MDL，**当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁**
- 事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放，这会导致即使修改一个小表都会阻塞。解决方法是避免长事务以及超时获取

## 行锁

- MySQL 的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如MyISAM 引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。
-  在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是**两阶段锁协议**。如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放
-  **Record Lock:** 对索引项加锁，锁定符合条件的行。其他事务不能修改和删除加锁项；
- **Gap Lock:** 对索引项之间的“间隙”加锁，锁定记录的范围（对第一条记录前的间隙或最后一条将记录后的间隙加锁），不包含索引项本身。其他事务不能在锁范围内插入数据，这样就防止了别的事务新增幻影行。
- **Next-key Lock：** 锁定索引项本身和索引范围。即Record Lock和Gap Lock的结合。可解决幻读问题。

## 死锁

- 当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。

- 死锁解决方法

  > 一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数innodb_lock_wait_timeout 来设置。在 InnoDB 中， innodb_lock_wait_timeout 的默认值是 50s
  > 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on ，表示开启这个逻辑。
  
- 解决由热点行更新导致死锁检测成为性能瓶颈的方法
  - 如果能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉。但是这种操作本身带有一定的风险，因为业务设计的时候一般不会把死锁当做一个严重错误，毕竟出现死锁了，就回滚，然后通过业务重试一般就没问题了，这是业务无损的。而关掉死锁检测意味着可能会出现大量的超时，这是业务有损的
  - 控制并发度（服务端控制而不是客户端控制）
  - 通过将一行改成逻辑上的多行来减少锁冲突（分段锁）

## 加锁规则（RR级别）

> 锁是加在索引上的，next-key lock分两个步骤，一是加间隙锁，一是加行锁，有可能导致死锁（一个事务想加NKL，但是只成功添加GAPLOCK，RECORDLOCK加锁失败）

### 两个“原则”、两个“优化”和一个“bug”

1. 原则1：加锁的基本单位是**next-key lock**，next-key lock是**前开后闭**区间。
2. 原则2：查找过程中访问到的对象才会加锁。
3. 优化1：索引上的**等值查询**，给**主键索引或者唯一索引**加锁的时候，next-key lock退化为行锁。
4. 优化2：索引上的**等值查询**，向右遍历时且最后一个值不满足等值条件的时候，next-key lock退化为间隙锁。
5. 一个bug：**唯一索引**上的**范围查询**会访问到不满足条件的第一个值为止。

### 覆盖索引

- lock in share mode只锁覆盖索引，但是如果是for update就不一样了。 执行 for update时，系统会认为你接下来要更新数据，因此会顺便给主键索引上满足条件的行加上行锁。；如果你要用lock in share mode来给行加读锁避免数据被更新的话，就必须得绕过覆盖索引的优化，在查询字段中加入索引中不存在的字段

# 事务

- begin/start transaction  命令并不是一个事务的起点，在执行到它们之后的第一个操作 InnoDB 表的语句，事务才真正启动。如果你想要马上启动一个事务，可以使用 start transaction with consistent snapshot  这个命令！！！

## 视图

在 MySQL 里，有两个 “ 视图 ” 的概念：

1. 一个是 view 。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。
   创建视图的语法是 create view …  ，而它的查询方法与表一样。
2. 另一个是 InnoDB 在实现 MVCC 时用到的一致性读视图，即 consistent read view ，用于支持
   RC （ Read Committed ，读提交）和 RR （ Repeatable Read ，可重复读）隔离级别的实现。
   它没有物理结构，作用是事务执行期间用来定义 “ 我能看到什么数据 ” 

- 在可重复读隔离级别下，事务在启动的时候就 “ 拍了个快照 ” 。注意，这个快照是基于整库的。

  > InnoDB 里面每个事务有一个唯一的事务 ID ，叫作 transaction id 。它是在事务开始的时候向InnoDB 的事务系统申请的，是按申请顺序严格递增的。
  > 而每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把 transaction id 赋值给这个数据版本的事务 ID ，记为 row trx_id 。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。
  > 也就是说，数据表中的一行记录，其实可能有多个版本 (row) ，每个版本有自己的 row trx_id 。

- InnoDB为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务ID。“活跃”指的就是，启动了但还没提交。每个事务也有自己独立的一致性视图

- 数组里面事务ID的最小值记为低水位，当前系统里面已经创建过的事务ID的最大值加1记为高水位。这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）。而数据版本的可见性规则，就是基于数据的row trx_id和这个一致性视图的对比结果得到的。
- **InnoDB利用了“所有数据都有多个版本”的这个特性，实现了“秒级创建快照”的能力，读取数据时从最新版本开始依次往后读取判断**
- 一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以外，有三种情况：
  1. 版本未提交，不可见；
  2. 版本已提交，但是是在视图创建后提交的，不可见；
  3. 版本已提交，而且是在视图创建前提交的，可见。
- **更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read），不然会导致其它事务的更新丢失**，除了update语句外，select语句如果加锁，也是当前读
- 表结构不支持“可重复读”？这是因为表结构没有对应的行数据，也没有row trx_id，因此只能遵循当前读的逻辑（MySQL 8.0已经可以把表结构放在InnoDB字典里了，也许以后会支持表结构的可重复读）
- 普通查询语句是一致性读，一致性读会根据row trx_id和一致性视图确定数据版本的可见性，可重复读的核心就是一致性读（consistent read）；而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待！！！
- 读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是：
  - 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图，查询只承认在**语句**启动前就已经提交完成的数据
  - 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图；查询只承认在**事务**启动前就已经提交完成的数据

# 解决表空洞

> 增删改都会使得数据表产生空洞（更新索引上的值，可以理解为删除一个旧的值，再插入一个新值），如果能够把这些空洞去掉，就能达到收缩表空间的目的

## 重建表

- `alter table A engine=InnoDB`，MySQL会创建一个临时表，将数据导入临时表，再替换（自动完成）
- alter语句在启动的时候需要获取MDL写锁，但是这个写锁在真正拷贝数据之前就退化成读锁了。为什么要退化呢？为了实现Online，MDL读锁不会阻塞增删改操作。那为什么不干脆直接解锁呢？为了保护自己，禁止其他线程对这个表同时做DDL

# Order By

## 全字段排序

- 将全部目标字段作为一个元素排序，有可能导致外部排序

## rowid排序

- 将排序的字段与主键ID作为一个元素排序，多了回表的操作，减少外部排序的临时文件数目

## 优先队列排序

- 即堆排序，适用于使用了`limit`子句的SQL，`filesort_priority_queue_optimization`，但是如果数据量超过了`sort_buffer_size`，则无法使用优先队列排序